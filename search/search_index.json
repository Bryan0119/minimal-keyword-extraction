{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"KeyBERT \u00b6 KeyBERT is a minimal and easy-to-use keyword extraction technique that leverages BERT embeddings to create keywords and keyphrases that are most similar to a document. About the Project \u00b6 Although there are already many methods available for keyword generation (e.g., Rake , YAKE! , TF-IDF, etc.) I wanted to create a very basic, but powerful method for extracting keywords and keyphrases. This is where KeyBERT comes in! Which uses BERT-embeddings and simple cosine similarity to find the sub-phrases in a document that are the most similar to the document itself. First, document embeddings are extracted with BERT to get a document-level representation. Then, word embeddings are extracted for N-gram words/phrases. Finally, we use cosine similarity to find the words/phrases that are the most similar to the document. The most similar words could then be identified as the words that best describe the entire document. KeyBERT is by no means unique and is created as a quick and easy method for creating keywords and keyphrases. Although there are many great papers and solutions out there that use BERT-embeddings (e.g., 1 , 2 , 3 , ), I could not find a BERT-based solution that did not have to be trained from scratch and could be used for beginners ( correct me if I'm wrong! ). Thus, the goal was a pip install keybert and at most 3 lines of code in usage. Installation \u00b6 Installation can be done using pypi : pip install keybert You may want to install more depending on the transformers and language backends that you will be using. The possible installations are: pip install keybert[flair] pip install keybert[gensim] pip install keybert[spacy] pip install keybert[use] Usage \u00b6 The most minimal example can be seen below for the extraction of keywords: from keybert import KeyBERT doc = \"\"\" Supervised learning is the machine learning task of learning a function that maps an input to an output based on example input-output pairs. It infers a function from labeled training data consisting of a set of training examples. In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a 'reasonable' way (see inductive bias). \"\"\" kw_model = KeyBERT () keywords = kw_model . extract_keywords ( doc ) You can set keyphrase_ngram_range to set the length of the resulting keywords/keyphrases: >>> kw_model . extract_keywords ( doc , keyphrase_ngram_range = ( 1 , 1 ), stop_words = None ) [( 'learning' , 0.4604 ), ( 'algorithm' , 0.4556 ), ( 'training' , 0.4487 ), ( 'class' , 0.4086 ), ( 'mapping' , 0.3700 )] To extract keyphrases, simply set keyphrase_ngram_range to (1, 2) or higher depending on the number of words you would like in the resulting keyphrases: >>> kw_model . extract_keywords ( doc , keyphrase_ngram_range = ( 1 , 2 ), stop_words = None ) [( 'learning algorithm' , 0.6978 ), ( 'machine learning' , 0.6305 ), ( 'supervised learning' , 0.5985 ), ( 'algorithm analyzes' , 0.5860 ), ( 'learning function' , 0.5850 )] NOTE You can also pass multiple documents at once if you are looking for a major speed-up!","title":"Home"},{"location":"index.html#keybert","text":"KeyBERT is a minimal and easy-to-use keyword extraction technique that leverages BERT embeddings to create keywords and keyphrases that are most similar to a document.","title":"KeyBERT"},{"location":"index.html#about-the-project","text":"Although there are already many methods available for keyword generation (e.g., Rake , YAKE! , TF-IDF, etc.) I wanted to create a very basic, but powerful method for extracting keywords and keyphrases. This is where KeyBERT comes in! Which uses BERT-embeddings and simple cosine similarity to find the sub-phrases in a document that are the most similar to the document itself. First, document embeddings are extracted with BERT to get a document-level representation. Then, word embeddings are extracted for N-gram words/phrases. Finally, we use cosine similarity to find the words/phrases that are the most similar to the document. The most similar words could then be identified as the words that best describe the entire document. KeyBERT is by no means unique and is created as a quick and easy method for creating keywords and keyphrases. Although there are many great papers and solutions out there that use BERT-embeddings (e.g., 1 , 2 , 3 , ), I could not find a BERT-based solution that did not have to be trained from scratch and could be used for beginners ( correct me if I'm wrong! ). Thus, the goal was a pip install keybert and at most 3 lines of code in usage.","title":"About the Project"},{"location":"index.html#installation","text":"Installation can be done using pypi : pip install keybert You may want to install more depending on the transformers and language backends that you will be using. The possible installations are: pip install keybert[flair] pip install keybert[gensim] pip install keybert[spacy] pip install keybert[use]","title":"Installation"},{"location":"index.html#usage","text":"The most minimal example can be seen below for the extraction of keywords: from keybert import KeyBERT doc = \"\"\" Supervised learning is the machine learning task of learning a function that maps an input to an output based on example input-output pairs. It infers a function from labeled training data consisting of a set of training examples. In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a 'reasonable' way (see inductive bias). \"\"\" kw_model = KeyBERT () keywords = kw_model . extract_keywords ( doc ) You can set keyphrase_ngram_range to set the length of the resulting keywords/keyphrases: >>> kw_model . extract_keywords ( doc , keyphrase_ngram_range = ( 1 , 1 ), stop_words = None ) [( 'learning' , 0.4604 ), ( 'algorithm' , 0.4556 ), ( 'training' , 0.4487 ), ( 'class' , 0.4086 ), ( 'mapping' , 0.3700 )] To extract keyphrases, simply set keyphrase_ngram_range to (1, 2) or higher depending on the number of words you would like in the resulting keyphrases: >>> kw_model . extract_keywords ( doc , keyphrase_ngram_range = ( 1 , 2 ), stop_words = None ) [( 'learning algorithm' , 0.6978 ), ( 'machine learning' , 0.6305 ), ( 'supervised learning' , 0.5985 ), ( 'algorithm analyzes' , 0.5860 ), ( 'learning function' , 0.5850 )] NOTE You can also pass multiple documents at once if you are looking for a major speed-up!","title":"Usage"},{"location":"changelog.html","text":"Version 0.7.0 \u00b6 Release date: 3 November, 2022 Highlights : Cleaned up documentation and added several visual representations of the algorithm (excluding MMR / MaxSum) Added function to extract and pass word- and document embeddings which should make fine-tuning much faster from keybert import KeyBERT kw_model = KeyBERT () # Prepare embeddings doc_embeddings , word_embeddings = kw_model . extract_embeddings ( docs ) # Extract keywords without needing to re-calculate embeddings keywords = kw_model . extract_keywords ( docs , doc_embeddings = doc_embeddings , word_embeddings = word_embeddings ) Do note that the parameters passed to .extract_embeddings for creating the vectorizer should be exactly the same as those in .extract_keywords . Fixes : Redundant documentation was removed by @mabhay3420 in #123 Fixed Gensim backend not working after v4 migration ( #71 ) Fixed candidates not working ( #122 ) Version 0.6.0 \u00b6 Release date: 25 July, 2022 Highlights : Major speedup, up to 2x to 5x when passing multiple documents (for MMR and MaxSum) compared to single documents Same results whether passing a single document or multiple documents MMR and MaxSum now work when passing a single document or multiple documents Improved documentation Added \ud83e\udd17 Hugging Face Transformers from keybert import KeyBERT from transformers.pipelines import pipeline hf_model = pipeline ( \"feature-extraction\" , model = \"distilbert-base-cased\" ) kw_model = KeyBERT ( model = hf_model ) Highlighting support for Chinese texts Now uses the CountVectorizer for creating the tokens This should also improve the highlighting for most applications and higher n-grams NOTE : Although highlighting for Chinese texts is improved, since I am not familiar with the Chinese language there is a good chance it is not yet as optimized as for other languages. Any feedback with respect to this is highly appreciated! Fixes : Fix typo in ReadMe by @priyanshul-govil in #117 Add missing optional dependencies (gensim, use, and spacy) by @yusuke1997 in #114 Version 0.5.1 \u00b6 Release date: 31 March, 2022 Added a page about leveraging CountVectorizer and KeyphraseVectorizers Shoutout to @TimSchopf for creating and optimizing the package! The KeyphraseVectorizers package can be found here Fixed Max Sum Similarity returning incorrect similarities #92 Thanks to @kunihik0 for the PR! Fixed out of bounds condition in MMR Thanks to @artmatsak for the PR! Started styling with Flake8 and Black (which was long overdue) Added pre-commit to make following through a bit easier with styling Version 0.5.0 \u00b6 Release date: 28 September, 2021 Highlights : Added Guided KeyBERT kw_model.extract_keywords(doc, seed_keywords=seed_keywords) Thanks to @zolekode for the inspiration! Use the newest all-* models from SBERT Miscellaneous : Added instructions in the FAQ to extract keywords from Chinese documents Fix typo in ReadMe by @koaning in #51 Version 0.4.0 \u00b6 Release date: 23 June, 2021 Highlights : Highlight a document's keywords with: keywords = kw_model.extract_keywords(doc, highlight=True) Use paraphrase-MiniLM-L6-v2 as the default embedder which gives great results! Miscellaneous : Update Flair dependencies Added FAQ Version 0.3.0 \u00b6 Release date: 10 May, 2021 The two main features are candidate keywords and several backends to use instead of Flair and SentenceTransformers! Highlights : Use candidate words instead of extracting those from the documents ( #25 ) KeyBERT().extract_keywords(doc, candidates) Spacy, Gensim, USE, and Custom Backends were added (see documentation here ) Fixes : Improved imports Fix encoding error when locally installing KeyBERT ( #30 ) Miscellaneous : Improved documentation (ReadMe & MKDocs) Add the main tutorial as a shield Typos ( #31 , #35 ) Version 0.2.0 \u00b6 Release date: 9 Feb, 2021 Highlights : Add similarity scores to the output Add Flair as a possible back-end Update documentation + improved testing Version 0.1.2 \u00b6 Release date: 28 Oct, 2020 Added Max Sum Similarity as an option to diversify your results. Version 0.1.0 \u00b6 Release date: 27 Oct, 2020 This first release includes keyword/keyphrase extraction using BERT and simple cosine similarity. There is also an option to use Maximal Marginal Relevance to select the candidate keywords/keyphrases.","title":"Changelog"},{"location":"changelog.html#version-070","text":"Release date: 3 November, 2022 Highlights : Cleaned up documentation and added several visual representations of the algorithm (excluding MMR / MaxSum) Added function to extract and pass word- and document embeddings which should make fine-tuning much faster from keybert import KeyBERT kw_model = KeyBERT () # Prepare embeddings doc_embeddings , word_embeddings = kw_model . extract_embeddings ( docs ) # Extract keywords without needing to re-calculate embeddings keywords = kw_model . extract_keywords ( docs , doc_embeddings = doc_embeddings , word_embeddings = word_embeddings ) Do note that the parameters passed to .extract_embeddings for creating the vectorizer should be exactly the same as those in .extract_keywords . Fixes : Redundant documentation was removed by @mabhay3420 in #123 Fixed Gensim backend not working after v4 migration ( #71 ) Fixed candidates not working ( #122 )","title":"Version 0.7.0"},{"location":"changelog.html#version-060","text":"Release date: 25 July, 2022 Highlights : Major speedup, up to 2x to 5x when passing multiple documents (for MMR and MaxSum) compared to single documents Same results whether passing a single document or multiple documents MMR and MaxSum now work when passing a single document or multiple documents Improved documentation Added \ud83e\udd17 Hugging Face Transformers from keybert import KeyBERT from transformers.pipelines import pipeline hf_model = pipeline ( \"feature-extraction\" , model = \"distilbert-base-cased\" ) kw_model = KeyBERT ( model = hf_model ) Highlighting support for Chinese texts Now uses the CountVectorizer for creating the tokens This should also improve the highlighting for most applications and higher n-grams NOTE : Although highlighting for Chinese texts is improved, since I am not familiar with the Chinese language there is a good chance it is not yet as optimized as for other languages. Any feedback with respect to this is highly appreciated! Fixes : Fix typo in ReadMe by @priyanshul-govil in #117 Add missing optional dependencies (gensim, use, and spacy) by @yusuke1997 in #114","title":"Version 0.6.0"},{"location":"changelog.html#version-051","text":"Release date: 31 March, 2022 Added a page about leveraging CountVectorizer and KeyphraseVectorizers Shoutout to @TimSchopf for creating and optimizing the package! The KeyphraseVectorizers package can be found here Fixed Max Sum Similarity returning incorrect similarities #92 Thanks to @kunihik0 for the PR! Fixed out of bounds condition in MMR Thanks to @artmatsak for the PR! Started styling with Flake8 and Black (which was long overdue) Added pre-commit to make following through a bit easier with styling","title":"Version 0.5.1"},{"location":"changelog.html#version-050","text":"Release date: 28 September, 2021 Highlights : Added Guided KeyBERT kw_model.extract_keywords(doc, seed_keywords=seed_keywords) Thanks to @zolekode for the inspiration! Use the newest all-* models from SBERT Miscellaneous : Added instructions in the FAQ to extract keywords from Chinese documents Fix typo in ReadMe by @koaning in #51","title":"Version 0.5.0"},{"location":"changelog.html#version-040","text":"Release date: 23 June, 2021 Highlights : Highlight a document's keywords with: keywords = kw_model.extract_keywords(doc, highlight=True) Use paraphrase-MiniLM-L6-v2 as the default embedder which gives great results! Miscellaneous : Update Flair dependencies Added FAQ","title":"Version 0.4.0"},{"location":"changelog.html#version-030","text":"Release date: 10 May, 2021 The two main features are candidate keywords and several backends to use instead of Flair and SentenceTransformers! Highlights : Use candidate words instead of extracting those from the documents ( #25 ) KeyBERT().extract_keywords(doc, candidates) Spacy, Gensim, USE, and Custom Backends were added (see documentation here ) Fixes : Improved imports Fix encoding error when locally installing KeyBERT ( #30 ) Miscellaneous : Improved documentation (ReadMe & MKDocs) Add the main tutorial as a shield Typos ( #31 , #35 )","title":"Version 0.3.0"},{"location":"changelog.html#version-020","text":"Release date: 9 Feb, 2021 Highlights : Add similarity scores to the output Add Flair as a possible back-end Update documentation + improved testing","title":"Version 0.2.0"},{"location":"changelog.html#version-012","text":"Release date: 28 Oct, 2020 Added Max Sum Similarity as an option to diversify your results.","title":"Version 0.1.2"},{"location":"changelog.html#version-010","text":"Release date: 27 Oct, 2020 This first release includes keyword/keyphrase extraction using BERT and simple cosine similarity. There is also an option to use Maximal Marginal Relevance to select the candidate keywords/keyphrases.","title":"Version 0.1.0"},{"location":"faq.html","text":"Which embedding model works best for which language? \u00b6 Unfortunately, there is not a definitive list of the best models for each language, this highly depends on your data, the model, and your specific use-case. However, the default model in KeyBERT ( \"all-MiniLM-L6-v2\" ) works great for English documents. In contrast, for multi-lingual documents or any other language, \"paraphrase-multilingual-MiniLM-L12-v2\"\" has shown great performance. If you want to use a model that provides a higher quality, but takes more compute time, then I would advise using paraphrase-mpnet-base-v2 and paraphrase-multilingual-mpnet-base-v2 instead. Should I preprocess the data? \u00b6 No. By using document embeddings there is typically no need to preprocess the data as all parts of a document are important in understanding the general topic of the document. Although this holds true in 99% of cases, if you have data that contains a lot of noise, for example, HTML-tags, then it would be best to remove them. HTML-tags typically do not contribute to the meaning of a document and should therefore be removed. However, if you apply topic modeling to HTML-code to extract topics of code, then it becomes important. How can I speed up the model? \u00b6 Since KeyBERT uses large language models as its backend, a GPU is typically prefered when using this package. Although it is possible to use it without a dedicated GPU, the inference speed will be significantly slower. A second method for speeding up KeyBERT is by passing it multiple documents at once. By doing this, words need to only be embedded a single time, which can result in a major speed up. This is faster : from keybert import KeyBERT kw_model = KeyBERT () keywords = kw_model . extract_keywords ( my_list_of_documents ) This is slower : from keybert import KeyBERT kw_model = KeyBERT () keywords = [] for document in my_list_of_documents : keyword = kw_model . extract_keywords ( document ) keywords . append ( keyword ) How can I use KeyBERT with Chinese documents? \u00b6 You need to make sure you use a tokenizer in KeyBERT that supports tokenization of Chinese. I suggest installing jieba for this: from sklearn.feature_extraction.text import CountVectorizer import jieba def tokenize_zh ( text ): words = jieba . lcut ( text ) return words vectorizer = CountVectorizer ( tokenizer = tokenize_zh ) Then, simply pass the vectorizer to your KeyBERT instance: from keybert import KeyBERT kw_model = KeyBERT () keywords = kw_model . extract_keywords ( doc , vectorizer = vectorizer ) It also supports highlighting:","title":"FAQ"},{"location":"faq.html#which-embedding-model-works-best-for-which-language","text":"Unfortunately, there is not a definitive list of the best models for each language, this highly depends on your data, the model, and your specific use-case. However, the default model in KeyBERT ( \"all-MiniLM-L6-v2\" ) works great for English documents. In contrast, for multi-lingual documents or any other language, \"paraphrase-multilingual-MiniLM-L12-v2\"\" has shown great performance. If you want to use a model that provides a higher quality, but takes more compute time, then I would advise using paraphrase-mpnet-base-v2 and paraphrase-multilingual-mpnet-base-v2 instead.","title":"Which embedding model works best for which language?"},{"location":"faq.html#should-i-preprocess-the-data","text":"No. By using document embeddings there is typically no need to preprocess the data as all parts of a document are important in understanding the general topic of the document. Although this holds true in 99% of cases, if you have data that contains a lot of noise, for example, HTML-tags, then it would be best to remove them. HTML-tags typically do not contribute to the meaning of a document and should therefore be removed. However, if you apply topic modeling to HTML-code to extract topics of code, then it becomes important.","title":"Should I preprocess the data?"},{"location":"faq.html#how-can-i-speed-up-the-model","text":"Since KeyBERT uses large language models as its backend, a GPU is typically prefered when using this package. Although it is possible to use it without a dedicated GPU, the inference speed will be significantly slower. A second method for speeding up KeyBERT is by passing it multiple documents at once. By doing this, words need to only be embedded a single time, which can result in a major speed up. This is faster : from keybert import KeyBERT kw_model = KeyBERT () keywords = kw_model . extract_keywords ( my_list_of_documents ) This is slower : from keybert import KeyBERT kw_model = KeyBERT () keywords = [] for document in my_list_of_documents : keyword = kw_model . extract_keywords ( document ) keywords . append ( keyword )","title":"How can I speed up the model?"},{"location":"faq.html#how-can-i-use-keybert-with-chinese-documents","text":"You need to make sure you use a tokenizer in KeyBERT that supports tokenization of Chinese. I suggest installing jieba for this: from sklearn.feature_extraction.text import CountVectorizer import jieba def tokenize_zh ( text ): words = jieba . lcut ( text ) return words vectorizer = CountVectorizer ( tokenizer = tokenize_zh ) Then, simply pass the vectorizer to your KeyBERT instance: from keybert import KeyBERT kw_model = KeyBERT () keywords = kw_model . extract_keywords ( doc , vectorizer = vectorizer ) It also supports highlighting:","title":"How can I use KeyBERT with Chinese documents?"},{"location":"api/keybert.html","text":"KeyBERT \u00b6 A minimal method for keyword extraction with BERT The keyword extraction is done by finding the sub-phrases in a document that are the most similar to the document itself. First, document embeddings are extracted with BERT to get a document-level representation. Then, word embeddings are extracted for N-gram words/phrases. Finally, we use cosine similarity to find the words/phrases that are the most similar to the document. The most similar words could then be identified as the words that best describe the entire document. @font-face { font-family: \"Virgil\"; src: url(\"https://unpkg.com/@excalidraw/excalidraw@0.12.0/dist/excalidraw-assets/Virgil.woff2\"); } @font-face { font-family: \"Cascadia\"; src: url(\"https://unpkg.com/@excalidraw/excalidraw@0.12.0/dist/excalidraw-assets/Cascadia.woff2\"); } Input Document Tokenize Words Embed Tokens Extract Embeddings Embed Document Calculate Cosine Similarity Most microbats use echolocationto navigate and find food. Most microbats use echolocationto navigate and find food. most microbats use echolocation to navigate and find food 0.11 0.55 0.28 .... .... .... 0.72 0.96 0.34 most food Most microbats... most food ... ... .08 .73 We use the CountVectorizer from Scikit-Learn to tokenize our document into candidate kewords/keyphrases. We can use any language model that can embed both documents and keywords, like sentence-transformers. We calculate the cosine similarity between all candidate keywords and the input document. The keywords that have the largest similarity to the document are extracted. Source code in keybert\\_model.py class KeyBERT : \"\"\" A minimal method for keyword extraction with BERT The keyword extraction is done by finding the sub-phrases in a document that are the most similar to the document itself. First, document embeddings are extracted with BERT to get a document-level representation. Then, word embeddings are extracted for N-gram words/phrases. Finally, we use cosine similarity to find the words/phrases that are the most similar to the document. The most similar words could then be identified as the words that best describe the entire document. <div class=\"excalidraw\"> --8<-- \"docs/images/pipeline.svg\" </div> \"\"\" def __init__ ( self , model = \"all-MiniLM-L6-v2\" ): \"\"\"KeyBERT initialization Arguments: model: Use a custom embedding model. The following backends are currently supported: * SentenceTransformers * \ud83e\udd17 Transformers * Flair * Spacy * Gensim * USE (TF-Hub) You can also pass in a string that points to one of the following sentence-transformers models: * https://www.sbert.net/docs/pretrained_models.html \"\"\" self . model = select_backend ( model ) def extract_keywords ( self , docs : Union [ str , List [ str ]], candidates : List [ str ] = None , keyphrase_ngram_range : Tuple [ int , int ] = ( 1 , 1 ), stop_words : Union [ str , List [ str ]] = \"english\" , top_n : int = 5 , min_df : int = 1 , use_maxsum : bool = False , use_mmr : bool = False , diversity : float = 0.5 , nr_candidates : int = 20 , vectorizer : CountVectorizer = None , highlight : bool = False , seed_keywords : List [ str ] = None , doc_embeddings : np . array = None , word_embeddings : np . array = None , ) -> Union [ List [ Tuple [ str , float ]], List [ List [ Tuple [ str , float ]]]]: \"\"\"Extract keywords and/or keyphrases To get the biggest speed-up, make sure to pass multiple documents at once instead of iterating over a single document. Arguments: docs: The document(s) for which to extract keywords/keyphrases candidates: Candidate keywords/keyphrases to use instead of extracting them from the document(s) NOTE: This is not used if you passed a `vectorizer`. keyphrase_ngram_range: Length, in words, of the extracted keywords/keyphrases. NOTE: This is not used if you passed a `vectorizer`. stop_words: Stopwords to remove from the document. NOTE: This is not used if you passed a `vectorizer`. top_n: Return the top n keywords/keyphrases min_df: Minimum document frequency of a word across all documents if keywords for multiple documents need to be extracted. NOTE: This is not used if you passed a `vectorizer`. use_maxsum: Whether to use Max Sum Distance for the selection of keywords/keyphrases. use_mmr: Whether to use Maximal Marginal Relevance (MMR) for the selection of keywords/keyphrases. diversity: The diversity of the results between 0 and 1 if `use_mmr` is set to True. nr_candidates: The number of candidates to consider if `use_maxsum` is set to True. vectorizer: Pass in your own `CountVectorizer` from `sklearn.feature_extraction.text.CountVectorizer` highlight: Whether to print the document and highlight its keywords/keyphrases. NOTE: This does not work if multiple documents are passed. seed_keywords: Seed keywords that may guide the extraction of keywords by steering the similarities towards the seeded keywords. doc_embeddings: The embeddings of each document. word_embeddings: The embeddings of each potential keyword/keyphrase across across the vocabulary of the set of input documents. NOTE: The `word_embeddings` should be generated through `.extract_embeddings` as the order of these embeddings depend on the vectorizer that was used to generate its vocabulary. Returns: keywords: The top n keywords for a document with their respective distances to the input document. Usage: To extract keywords from a single document: ```python from keybert import KeyBERT kw_model = KeyBERT() keywords = kw_model.extract_keywords(doc) ``` To extract keywords from multiple documents, which is typically quite a bit faster: ```python from keybert import KeyBERT kw_model = KeyBERT() keywords = kw_model.extract_keywords(docs) ``` \"\"\" # Check for a single, empty document if isinstance ( docs , str ): if docs : docs = [ docs ] else : return [] # Extract potential words using a vectorizer / tokenizer if vectorizer : count = vectorizer . fit ( docs ) else : try : count = CountVectorizer ( ngram_range = keyphrase_ngram_range , stop_words = stop_words , min_df = min_df , vocabulary = candidates , ) . fit ( docs ) except ValueError : return [] # Scikit-Learn Deprecation: get_feature_names is deprecated in 1.0 # and will be removed in 1.2. Please use get_feature_names_out instead. if version . parse ( sklearn_version ) >= version . parse ( \"1.0.0\" ): words = count . get_feature_names_out () else : words = count . get_feature_names () df = count . transform ( docs ) # Check if the right number of word embeddings are generated compared with the vectorizer if word_embeddings is not None : if word_embeddings . shape [ 0 ] != len ( words ): raise ValueError ( \"Make sure that the `word_embeddings` are generated from the function \" \"`.extract_embeddings`. \\n Moreover, the `candidates`, `keyphrase_ngram_range`,\" \"`stop_words`, and `min_df` parameters need to have the same values in both \" \"`.extract_embeddings` and `.extract_keywords`.\" ) # Extract embeddings if doc_embeddings is None : doc_embeddings = self . model . embed ( docs ) if word_embeddings is None : word_embeddings = self . model . embed ( words ) if seed_keywords is not None : seed_embeddings = self . model . embed ([ \" \" . join ( seed_keywords )]) # Find keywords all_keywords = [] for index , _ in enumerate ( docs ): try : # Select embeddings candidate_indices = df [ index ] . nonzero ()[ 1 ] candidates = [ words [ index ] for index in candidate_indices ] candidate_embeddings = word_embeddings [ candidate_indices ] doc_embedding = doc_embeddings [ index ] . reshape ( 1 , - 1 ) # Guided KeyBERT with seed keywords if seed_keywords is not None : doc_embedding = np . average ( [ doc_embedding , seed_embeddings ], axis = 0 , weights = [ 3 , 1 ] ) # Maximal Marginal Relevance (MMR) if use_mmr : keywords = mmr ( doc_embedding , candidate_embeddings , candidates , top_n , diversity , ) # Max Sum Distance elif use_maxsum : keywords = max_sum_distance ( doc_embedding , candidate_embeddings , candidates , top_n , nr_candidates , ) # Cosine-based keyword extraction else : distances = cosine_similarity ( doc_embedding , candidate_embeddings ) keywords = [ ( candidates [ index ], round ( float ( distances [ 0 ][ index ]), 4 )) for index in distances . argsort ()[ 0 ][ - top_n :] ][:: - 1 ] all_keywords . append ( keywords ) # Capturing empty keywords except ValueError : all_keywords . append ([]) # Highlight keywords in the document if len ( all_keywords ) == 1 : if highlight : highlight_document ( docs [ 0 ], all_keywords [ 0 ], count ) all_keywords = all_keywords [ 0 ] return all_keywords def extract_embeddings ( self , docs : Union [ str , List [ str ]], candidates : List [ str ] = None , keyphrase_ngram_range : Tuple [ int , int ] = ( 1 , 1 ), stop_words : Union [ str , List [ str ]] = \"english\" , min_df : int = 1 , vectorizer : CountVectorizer = None ) -> Union [ List [ Tuple [ str , float ]], List [ List [ Tuple [ str , float ]]]]: \"\"\"Extract document and word embeddings for the input documents and the generated candidate keywords/keyphrases respectively. Note that all potential keywords/keyphrases are not returned but only their word embeddings. This means that the values of `candidates`, `keyphrase_ngram_range`, `stop_words`, and `min_df` need to be the same between using `.extract_embeddings` and `.extract_keywords`. Arguments: docs: The document(s) for which to extract keywords/keyphrases candidates: Candidate keywords/keyphrases to use instead of extracting them from the document(s) NOTE: This is not used if you passed a `vectorizer`. keyphrase_ngram_range: Length, in words, of the extracted keywords/keyphrases. NOTE: This is not used if you passed a `vectorizer`. stop_words: Stopwords to remove from the document. NOTE: This is not used if you passed a `vectorizer`. min_df: Minimum document frequency of a word across all documents if keywords for multiple documents need to be extracted. NOTE: This is not used if you passed a `vectorizer`. vectorizer: Pass in your own `CountVectorizer` from `sklearn.feature_extraction.text.CountVectorizer` Returns: doc_embeddings: The embeddings of each document. word_embeddings: The embeddings of each potential keyword/keyphrase across across the vocabulary of the set of input documents. NOTE: The `word_embeddings` should be generated through `.extract_embeddings` as the order of these embeddings depend on the vectorizer that was used to generate its vocabulary. Usage: To generate the word and document embeddings from a set of documents: ```python from keybert import KeyBERT kw_model = KeyBERT() doc_embeddings, word_embeddings = kw_model.extract_embeddings(docs) ``` You can then use these embeddings and pass them to `.extract_keywords` to speed up the tuning the model: ```python keywords = kw_model.extract_keywords(docs, doc_embeddings=doc_embeddings, word_embeddings=word_embeddings) ``` \"\"\" # Check for a single, empty document if isinstance ( docs , str ): if docs : docs = [ docs ] else : return [] # Extract potential words using a vectorizer / tokenizer if vectorizer : count = vectorizer . fit ( docs ) else : try : count = CountVectorizer ( ngram_range = keyphrase_ngram_range , stop_words = stop_words , min_df = min_df , vocabulary = candidates , ) . fit ( docs ) except ValueError : return [] # Scikit-Learn Deprecation: get_feature_names is deprecated in 1.0 # and will be removed in 1.2. Please use get_feature_names_out instead. if version . parse ( sklearn_version ) >= version . parse ( \"1.0.0\" ): words = count . get_feature_names_out () else : words = count . get_feature_names () doc_embeddings = self . model . embed ( docs ) word_embeddings = self . model . embed ( words ) return doc_embeddings , word_embeddings __init__ ( self , model = 'all-MiniLM-L6-v2' ) special \u00b6 KeyBERT initialization Parameters: Name Type Description Default model Use a custom embedding model. The following backends are currently supported: * SentenceTransformers * \ud83e\udd17 Transformers * Flair * Spacy * Gensim * USE (TF-Hub) You can also pass in a string that points to one of the following sentence-transformers models: * https://www.sbert.net/docs/pretrained_models.html 'all-MiniLM-L6-v2' Source code in keybert\\_model.py def __init__ ( self , model = \"all-MiniLM-L6-v2\" ): \"\"\"KeyBERT initialization Arguments: model: Use a custom embedding model. The following backends are currently supported: * SentenceTransformers * \ud83e\udd17 Transformers * Flair * Spacy * Gensim * USE (TF-Hub) You can also pass in a string that points to one of the following sentence-transformers models: * https://www.sbert.net/docs/pretrained_models.html \"\"\" self . model = select_backend ( model ) extract_embeddings ( self , docs , candidates = None , keyphrase_ngram_range = ( 1 , 1 ), stop_words = 'english' , min_df = 1 , vectorizer = None ) \u00b6 Extract document and word embeddings for the input documents and the generated candidate keywords/keyphrases respectively. Note that all potential keywords/keyphrases are not returned but only their word embeddings. This means that the values of candidates , keyphrase_ngram_range , stop_words , and min_df need to be the same between using .extract_embeddings and .extract_keywords . Parameters: Name Type Description Default docs Union[str, List[str]] The document(s) for which to extract keywords/keyphrases required candidates List[str] Candidate keywords/keyphrases to use instead of extracting them from the document(s) NOTE: This is not used if you passed a vectorizer . None keyphrase_ngram_range Tuple[int, int] Length, in words, of the extracted keywords/keyphrases. NOTE: This is not used if you passed a vectorizer . (1, 1) stop_words Union[str, List[str]] Stopwords to remove from the document. NOTE: This is not used if you passed a vectorizer . 'english' min_df int Minimum document frequency of a word across all documents if keywords for multiple documents need to be extracted. NOTE: This is not used if you passed a vectorizer . 1 vectorizer CountVectorizer Pass in your own CountVectorizer from sklearn.feature_extraction.text.CountVectorizer None Returns: Type Description doc_embeddings The embeddings of each document. word_embeddings: The embeddings of each potential keyword/keyphrase across across the vocabulary of the set of input documents. NOTE: The word_embeddings should be generated through .extract_embeddings as the order of these embeddings depend on the vectorizer that was used to generate its vocabulary. Usage: To generate the word and document embeddings from a set of documents: from keybert import KeyBERT kw_model = KeyBERT () doc_embeddings , word_embeddings = kw_model . extract_embeddings ( docs ) You can then use these embeddings and pass them to .extract_keywords to speed up the tuning the model: keywords = kw_model . extract_keywords ( docs , doc_embeddings = doc_embeddings , word_embeddings = word_embeddings ) Source code in keybert\\_model.py def extract_embeddings ( self , docs : Union [ str , List [ str ]], candidates : List [ str ] = None , keyphrase_ngram_range : Tuple [ int , int ] = ( 1 , 1 ), stop_words : Union [ str , List [ str ]] = \"english\" , min_df : int = 1 , vectorizer : CountVectorizer = None ) -> Union [ List [ Tuple [ str , float ]], List [ List [ Tuple [ str , float ]]]]: \"\"\"Extract document and word embeddings for the input documents and the generated candidate keywords/keyphrases respectively. Note that all potential keywords/keyphrases are not returned but only their word embeddings. This means that the values of `candidates`, `keyphrase_ngram_range`, `stop_words`, and `min_df` need to be the same between using `.extract_embeddings` and `.extract_keywords`. Arguments: docs: The document(s) for which to extract keywords/keyphrases candidates: Candidate keywords/keyphrases to use instead of extracting them from the document(s) NOTE: This is not used if you passed a `vectorizer`. keyphrase_ngram_range: Length, in words, of the extracted keywords/keyphrases. NOTE: This is not used if you passed a `vectorizer`. stop_words: Stopwords to remove from the document. NOTE: This is not used if you passed a `vectorizer`. min_df: Minimum document frequency of a word across all documents if keywords for multiple documents need to be extracted. NOTE: This is not used if you passed a `vectorizer`. vectorizer: Pass in your own `CountVectorizer` from `sklearn.feature_extraction.text.CountVectorizer` Returns: doc_embeddings: The embeddings of each document. word_embeddings: The embeddings of each potential keyword/keyphrase across across the vocabulary of the set of input documents. NOTE: The `word_embeddings` should be generated through `.extract_embeddings` as the order of these embeddings depend on the vectorizer that was used to generate its vocabulary. Usage: To generate the word and document embeddings from a set of documents: ```python from keybert import KeyBERT kw_model = KeyBERT() doc_embeddings, word_embeddings = kw_model.extract_embeddings(docs) ``` You can then use these embeddings and pass them to `.extract_keywords` to speed up the tuning the model: ```python keywords = kw_model.extract_keywords(docs, doc_embeddings=doc_embeddings, word_embeddings=word_embeddings) ``` \"\"\" # Check for a single, empty document if isinstance ( docs , str ): if docs : docs = [ docs ] else : return [] # Extract potential words using a vectorizer / tokenizer if vectorizer : count = vectorizer . fit ( docs ) else : try : count = CountVectorizer ( ngram_range = keyphrase_ngram_range , stop_words = stop_words , min_df = min_df , vocabulary = candidates , ) . fit ( docs ) except ValueError : return [] # Scikit-Learn Deprecation: get_feature_names is deprecated in 1.0 # and will be removed in 1.2. Please use get_feature_names_out instead. if version . parse ( sklearn_version ) >= version . parse ( \"1.0.0\" ): words = count . get_feature_names_out () else : words = count . get_feature_names () doc_embeddings = self . model . embed ( docs ) word_embeddings = self . model . embed ( words ) return doc_embeddings , word_embeddings extract_keywords ( self , docs , candidates = None , keyphrase_ngram_range = ( 1 , 1 ), stop_words = 'english' , top_n = 5 , min_df = 1 , use_maxsum = False , use_mmr = False , diversity = 0.5 , nr_candidates = 20 , vectorizer = None , highlight = False , seed_keywords = None , doc_embeddings = None , word_embeddings = None ) \u00b6 Extract keywords and/or keyphrases To get the biggest speed-up, make sure to pass multiple documents at once instead of iterating over a single document. Parameters: Name Type Description Default docs Union[str, List[str]] The document(s) for which to extract keywords/keyphrases required candidates List[str] Candidate keywords/keyphrases to use instead of extracting them from the document(s) NOTE: This is not used if you passed a vectorizer . None keyphrase_ngram_range Tuple[int, int] Length, in words, of the extracted keywords/keyphrases. NOTE: This is not used if you passed a vectorizer . (1, 1) stop_words Union[str, List[str]] Stopwords to remove from the document. NOTE: This is not used if you passed a vectorizer . 'english' top_n int Return the top n keywords/keyphrases 5 min_df int Minimum document frequency of a word across all documents if keywords for multiple documents need to be extracted. NOTE: This is not used if you passed a vectorizer . 1 use_maxsum bool Whether to use Max Sum Distance for the selection of keywords/keyphrases. False use_mmr bool Whether to use Maximal Marginal Relevance (MMR) for the selection of keywords/keyphrases. False diversity float The diversity of the results between 0 and 1 if use_mmr is set to True. 0.5 nr_candidates int The number of candidates to consider if use_maxsum is set to True. 20 vectorizer CountVectorizer Pass in your own CountVectorizer from sklearn.feature_extraction.text.CountVectorizer None highlight bool Whether to print the document and highlight its keywords/keyphrases. NOTE: This does not work if multiple documents are passed. False seed_keywords List[str] Seed keywords that may guide the extraction of keywords by steering the similarities towards the seeded keywords. None doc_embeddings <built-in function array> The embeddings of each document. None word_embeddings <built-in function array> The embeddings of each potential keyword/keyphrase across across the vocabulary of the set of input documents. NOTE: The word_embeddings should be generated through .extract_embeddings as the order of these embeddings depend on the vectorizer that was used to generate its vocabulary. None Returns: Type Description keywords The top n keywords for a document with their respective distances to the input document. Usage: To extract keywords from a single document: from keybert import KeyBERT kw_model = KeyBERT () keywords = kw_model . extract_keywords ( doc ) To extract keywords from multiple documents, which is typically quite a bit faster: from keybert import KeyBERT kw_model = KeyBERT () keywords = kw_model . extract_keywords ( docs ) Source code in keybert\\_model.py def extract_keywords ( self , docs : Union [ str , List [ str ]], candidates : List [ str ] = None , keyphrase_ngram_range : Tuple [ int , int ] = ( 1 , 1 ), stop_words : Union [ str , List [ str ]] = \"english\" , top_n : int = 5 , min_df : int = 1 , use_maxsum : bool = False , use_mmr : bool = False , diversity : float = 0.5 , nr_candidates : int = 20 , vectorizer : CountVectorizer = None , highlight : bool = False , seed_keywords : List [ str ] = None , doc_embeddings : np . array = None , word_embeddings : np . array = None , ) -> Union [ List [ Tuple [ str , float ]], List [ List [ Tuple [ str , float ]]]]: \"\"\"Extract keywords and/or keyphrases To get the biggest speed-up, make sure to pass multiple documents at once instead of iterating over a single document. Arguments: docs: The document(s) for which to extract keywords/keyphrases candidates: Candidate keywords/keyphrases to use instead of extracting them from the document(s) NOTE: This is not used if you passed a `vectorizer`. keyphrase_ngram_range: Length, in words, of the extracted keywords/keyphrases. NOTE: This is not used if you passed a `vectorizer`. stop_words: Stopwords to remove from the document. NOTE: This is not used if you passed a `vectorizer`. top_n: Return the top n keywords/keyphrases min_df: Minimum document frequency of a word across all documents if keywords for multiple documents need to be extracted. NOTE: This is not used if you passed a `vectorizer`. use_maxsum: Whether to use Max Sum Distance for the selection of keywords/keyphrases. use_mmr: Whether to use Maximal Marginal Relevance (MMR) for the selection of keywords/keyphrases. diversity: The diversity of the results between 0 and 1 if `use_mmr` is set to True. nr_candidates: The number of candidates to consider if `use_maxsum` is set to True. vectorizer: Pass in your own `CountVectorizer` from `sklearn.feature_extraction.text.CountVectorizer` highlight: Whether to print the document and highlight its keywords/keyphrases. NOTE: This does not work if multiple documents are passed. seed_keywords: Seed keywords that may guide the extraction of keywords by steering the similarities towards the seeded keywords. doc_embeddings: The embeddings of each document. word_embeddings: The embeddings of each potential keyword/keyphrase across across the vocabulary of the set of input documents. NOTE: The `word_embeddings` should be generated through `.extract_embeddings` as the order of these embeddings depend on the vectorizer that was used to generate its vocabulary. Returns: keywords: The top n keywords for a document with their respective distances to the input document. Usage: To extract keywords from a single document: ```python from keybert import KeyBERT kw_model = KeyBERT() keywords = kw_model.extract_keywords(doc) ``` To extract keywords from multiple documents, which is typically quite a bit faster: ```python from keybert import KeyBERT kw_model = KeyBERT() keywords = kw_model.extract_keywords(docs) ``` \"\"\" # Check for a single, empty document if isinstance ( docs , str ): if docs : docs = [ docs ] else : return [] # Extract potential words using a vectorizer / tokenizer if vectorizer : count = vectorizer . fit ( docs ) else : try : count = CountVectorizer ( ngram_range = keyphrase_ngram_range , stop_words = stop_words , min_df = min_df , vocabulary = candidates , ) . fit ( docs ) except ValueError : return [] # Scikit-Learn Deprecation: get_feature_names is deprecated in 1.0 # and will be removed in 1.2. Please use get_feature_names_out instead. if version . parse ( sklearn_version ) >= version . parse ( \"1.0.0\" ): words = count . get_feature_names_out () else : words = count . get_feature_names () df = count . transform ( docs ) # Check if the right number of word embeddings are generated compared with the vectorizer if word_embeddings is not None : if word_embeddings . shape [ 0 ] != len ( words ): raise ValueError ( \"Make sure that the `word_embeddings` are generated from the function \" \"`.extract_embeddings`. \\n Moreover, the `candidates`, `keyphrase_ngram_range`,\" \"`stop_words`, and `min_df` parameters need to have the same values in both \" \"`.extract_embeddings` and `.extract_keywords`.\" ) # Extract embeddings if doc_embeddings is None : doc_embeddings = self . model . embed ( docs ) if word_embeddings is None : word_embeddings = self . model . embed ( words ) if seed_keywords is not None : seed_embeddings = self . model . embed ([ \" \" . join ( seed_keywords )]) # Find keywords all_keywords = [] for index , _ in enumerate ( docs ): try : # Select embeddings candidate_indices = df [ index ] . nonzero ()[ 1 ] candidates = [ words [ index ] for index in candidate_indices ] candidate_embeddings = word_embeddings [ candidate_indices ] doc_embedding = doc_embeddings [ index ] . reshape ( 1 , - 1 ) # Guided KeyBERT with seed keywords if seed_keywords is not None : doc_embedding = np . average ( [ doc_embedding , seed_embeddings ], axis = 0 , weights = [ 3 , 1 ] ) # Maximal Marginal Relevance (MMR) if use_mmr : keywords = mmr ( doc_embedding , candidate_embeddings , candidates , top_n , diversity , ) # Max Sum Distance elif use_maxsum : keywords = max_sum_distance ( doc_embedding , candidate_embeddings , candidates , top_n , nr_candidates , ) # Cosine-based keyword extraction else : distances = cosine_similarity ( doc_embedding , candidate_embeddings ) keywords = [ ( candidates [ index ], round ( float ( distances [ 0 ][ index ]), 4 )) for index in distances . argsort ()[ 0 ][ - top_n :] ][:: - 1 ] all_keywords . append ( keywords ) # Capturing empty keywords except ValueError : all_keywords . append ([]) # Highlight keywords in the document if len ( all_keywords ) == 1 : if highlight : highlight_document ( docs [ 0 ], all_keywords [ 0 ], count ) all_keywords = all_keywords [ 0 ] return all_keywords","title":"KeyBERT"},{"location":"api/keybert.html#keybert","text":"A minimal method for keyword extraction with BERT The keyword extraction is done by finding the sub-phrases in a document that are the most similar to the document itself. First, document embeddings are extracted with BERT to get a document-level representation. Then, word embeddings are extracted for N-gram words/phrases. Finally, we use cosine similarity to find the words/phrases that are the most similar to the document. The most similar words could then be identified as the words that best describe the entire document. @font-face { font-family: \"Virgil\"; src: url(\"https://unpkg.com/@excalidraw/excalidraw@0.12.0/dist/excalidraw-assets/Virgil.woff2\"); } @font-face { font-family: \"Cascadia\"; src: url(\"https://unpkg.com/@excalidraw/excalidraw@0.12.0/dist/excalidraw-assets/Cascadia.woff2\"); } Input Document Tokenize Words Embed Tokens Extract Embeddings Embed Document Calculate Cosine Similarity Most microbats use echolocationto navigate and find food. Most microbats use echolocationto navigate and find food. most microbats use echolocation to navigate and find food 0.11 0.55 0.28 .... .... .... 0.72 0.96 0.34 most food Most microbats... most food ... ... .08 .73 We use the CountVectorizer from Scikit-Learn to tokenize our document into candidate kewords/keyphrases. We can use any language model that can embed both documents and keywords, like sentence-transformers. We calculate the cosine similarity between all candidate keywords and the input document. The keywords that have the largest similarity to the document are extracted. Source code in keybert\\_model.py class KeyBERT : \"\"\" A minimal method for keyword extraction with BERT The keyword extraction is done by finding the sub-phrases in a document that are the most similar to the document itself. First, document embeddings are extracted with BERT to get a document-level representation. Then, word embeddings are extracted for N-gram words/phrases. Finally, we use cosine similarity to find the words/phrases that are the most similar to the document. The most similar words could then be identified as the words that best describe the entire document. <div class=\"excalidraw\"> --8<-- \"docs/images/pipeline.svg\" </div> \"\"\" def __init__ ( self , model = \"all-MiniLM-L6-v2\" ): \"\"\"KeyBERT initialization Arguments: model: Use a custom embedding model. The following backends are currently supported: * SentenceTransformers * \ud83e\udd17 Transformers * Flair * Spacy * Gensim * USE (TF-Hub) You can also pass in a string that points to one of the following sentence-transformers models: * https://www.sbert.net/docs/pretrained_models.html \"\"\" self . model = select_backend ( model ) def extract_keywords ( self , docs : Union [ str , List [ str ]], candidates : List [ str ] = None , keyphrase_ngram_range : Tuple [ int , int ] = ( 1 , 1 ), stop_words : Union [ str , List [ str ]] = \"english\" , top_n : int = 5 , min_df : int = 1 , use_maxsum : bool = False , use_mmr : bool = False , diversity : float = 0.5 , nr_candidates : int = 20 , vectorizer : CountVectorizer = None , highlight : bool = False , seed_keywords : List [ str ] = None , doc_embeddings : np . array = None , word_embeddings : np . array = None , ) -> Union [ List [ Tuple [ str , float ]], List [ List [ Tuple [ str , float ]]]]: \"\"\"Extract keywords and/or keyphrases To get the biggest speed-up, make sure to pass multiple documents at once instead of iterating over a single document. Arguments: docs: The document(s) for which to extract keywords/keyphrases candidates: Candidate keywords/keyphrases to use instead of extracting them from the document(s) NOTE: This is not used if you passed a `vectorizer`. keyphrase_ngram_range: Length, in words, of the extracted keywords/keyphrases. NOTE: This is not used if you passed a `vectorizer`. stop_words: Stopwords to remove from the document. NOTE: This is not used if you passed a `vectorizer`. top_n: Return the top n keywords/keyphrases min_df: Minimum document frequency of a word across all documents if keywords for multiple documents need to be extracted. NOTE: This is not used if you passed a `vectorizer`. use_maxsum: Whether to use Max Sum Distance for the selection of keywords/keyphrases. use_mmr: Whether to use Maximal Marginal Relevance (MMR) for the selection of keywords/keyphrases. diversity: The diversity of the results between 0 and 1 if `use_mmr` is set to True. nr_candidates: The number of candidates to consider if `use_maxsum` is set to True. vectorizer: Pass in your own `CountVectorizer` from `sklearn.feature_extraction.text.CountVectorizer` highlight: Whether to print the document and highlight its keywords/keyphrases. NOTE: This does not work if multiple documents are passed. seed_keywords: Seed keywords that may guide the extraction of keywords by steering the similarities towards the seeded keywords. doc_embeddings: The embeddings of each document. word_embeddings: The embeddings of each potential keyword/keyphrase across across the vocabulary of the set of input documents. NOTE: The `word_embeddings` should be generated through `.extract_embeddings` as the order of these embeddings depend on the vectorizer that was used to generate its vocabulary. Returns: keywords: The top n keywords for a document with their respective distances to the input document. Usage: To extract keywords from a single document: ```python from keybert import KeyBERT kw_model = KeyBERT() keywords = kw_model.extract_keywords(doc) ``` To extract keywords from multiple documents, which is typically quite a bit faster: ```python from keybert import KeyBERT kw_model = KeyBERT() keywords = kw_model.extract_keywords(docs) ``` \"\"\" # Check for a single, empty document if isinstance ( docs , str ): if docs : docs = [ docs ] else : return [] # Extract potential words using a vectorizer / tokenizer if vectorizer : count = vectorizer . fit ( docs ) else : try : count = CountVectorizer ( ngram_range = keyphrase_ngram_range , stop_words = stop_words , min_df = min_df , vocabulary = candidates , ) . fit ( docs ) except ValueError : return [] # Scikit-Learn Deprecation: get_feature_names is deprecated in 1.0 # and will be removed in 1.2. Please use get_feature_names_out instead. if version . parse ( sklearn_version ) >= version . parse ( \"1.0.0\" ): words = count . get_feature_names_out () else : words = count . get_feature_names () df = count . transform ( docs ) # Check if the right number of word embeddings are generated compared with the vectorizer if word_embeddings is not None : if word_embeddings . shape [ 0 ] != len ( words ): raise ValueError ( \"Make sure that the `word_embeddings` are generated from the function \" \"`.extract_embeddings`. \\n Moreover, the `candidates`, `keyphrase_ngram_range`,\" \"`stop_words`, and `min_df` parameters need to have the same values in both \" \"`.extract_embeddings` and `.extract_keywords`.\" ) # Extract embeddings if doc_embeddings is None : doc_embeddings = self . model . embed ( docs ) if word_embeddings is None : word_embeddings = self . model . embed ( words ) if seed_keywords is not None : seed_embeddings = self . model . embed ([ \" \" . join ( seed_keywords )]) # Find keywords all_keywords = [] for index , _ in enumerate ( docs ): try : # Select embeddings candidate_indices = df [ index ] . nonzero ()[ 1 ] candidates = [ words [ index ] for index in candidate_indices ] candidate_embeddings = word_embeddings [ candidate_indices ] doc_embedding = doc_embeddings [ index ] . reshape ( 1 , - 1 ) # Guided KeyBERT with seed keywords if seed_keywords is not None : doc_embedding = np . average ( [ doc_embedding , seed_embeddings ], axis = 0 , weights = [ 3 , 1 ] ) # Maximal Marginal Relevance (MMR) if use_mmr : keywords = mmr ( doc_embedding , candidate_embeddings , candidates , top_n , diversity , ) # Max Sum Distance elif use_maxsum : keywords = max_sum_distance ( doc_embedding , candidate_embeddings , candidates , top_n , nr_candidates , ) # Cosine-based keyword extraction else : distances = cosine_similarity ( doc_embedding , candidate_embeddings ) keywords = [ ( candidates [ index ], round ( float ( distances [ 0 ][ index ]), 4 )) for index in distances . argsort ()[ 0 ][ - top_n :] ][:: - 1 ] all_keywords . append ( keywords ) # Capturing empty keywords except ValueError : all_keywords . append ([]) # Highlight keywords in the document if len ( all_keywords ) == 1 : if highlight : highlight_document ( docs [ 0 ], all_keywords [ 0 ], count ) all_keywords = all_keywords [ 0 ] return all_keywords def extract_embeddings ( self , docs : Union [ str , List [ str ]], candidates : List [ str ] = None , keyphrase_ngram_range : Tuple [ int , int ] = ( 1 , 1 ), stop_words : Union [ str , List [ str ]] = \"english\" , min_df : int = 1 , vectorizer : CountVectorizer = None ) -> Union [ List [ Tuple [ str , float ]], List [ List [ Tuple [ str , float ]]]]: \"\"\"Extract document and word embeddings for the input documents and the generated candidate keywords/keyphrases respectively. Note that all potential keywords/keyphrases are not returned but only their word embeddings. This means that the values of `candidates`, `keyphrase_ngram_range`, `stop_words`, and `min_df` need to be the same between using `.extract_embeddings` and `.extract_keywords`. Arguments: docs: The document(s) for which to extract keywords/keyphrases candidates: Candidate keywords/keyphrases to use instead of extracting them from the document(s) NOTE: This is not used if you passed a `vectorizer`. keyphrase_ngram_range: Length, in words, of the extracted keywords/keyphrases. NOTE: This is not used if you passed a `vectorizer`. stop_words: Stopwords to remove from the document. NOTE: This is not used if you passed a `vectorizer`. min_df: Minimum document frequency of a word across all documents if keywords for multiple documents need to be extracted. NOTE: This is not used if you passed a `vectorizer`. vectorizer: Pass in your own `CountVectorizer` from `sklearn.feature_extraction.text.CountVectorizer` Returns: doc_embeddings: The embeddings of each document. word_embeddings: The embeddings of each potential keyword/keyphrase across across the vocabulary of the set of input documents. NOTE: The `word_embeddings` should be generated through `.extract_embeddings` as the order of these embeddings depend on the vectorizer that was used to generate its vocabulary. Usage: To generate the word and document embeddings from a set of documents: ```python from keybert import KeyBERT kw_model = KeyBERT() doc_embeddings, word_embeddings = kw_model.extract_embeddings(docs) ``` You can then use these embeddings and pass them to `.extract_keywords` to speed up the tuning the model: ```python keywords = kw_model.extract_keywords(docs, doc_embeddings=doc_embeddings, word_embeddings=word_embeddings) ``` \"\"\" # Check for a single, empty document if isinstance ( docs , str ): if docs : docs = [ docs ] else : return [] # Extract potential words using a vectorizer / tokenizer if vectorizer : count = vectorizer . fit ( docs ) else : try : count = CountVectorizer ( ngram_range = keyphrase_ngram_range , stop_words = stop_words , min_df = min_df , vocabulary = candidates , ) . fit ( docs ) except ValueError : return [] # Scikit-Learn Deprecation: get_feature_names is deprecated in 1.0 # and will be removed in 1.2. Please use get_feature_names_out instead. if version . parse ( sklearn_version ) >= version . parse ( \"1.0.0\" ): words = count . get_feature_names_out () else : words = count . get_feature_names () doc_embeddings = self . model . embed ( docs ) word_embeddings = self . model . embed ( words ) return doc_embeddings , word_embeddings","title":"KeyBERT"},{"location":"api/keybert.html#keybert._model.KeyBERT.__init__","text":"KeyBERT initialization Parameters: Name Type Description Default model Use a custom embedding model. The following backends are currently supported: * SentenceTransformers * \ud83e\udd17 Transformers * Flair * Spacy * Gensim * USE (TF-Hub) You can also pass in a string that points to one of the following sentence-transformers models: * https://www.sbert.net/docs/pretrained_models.html 'all-MiniLM-L6-v2' Source code in keybert\\_model.py def __init__ ( self , model = \"all-MiniLM-L6-v2\" ): \"\"\"KeyBERT initialization Arguments: model: Use a custom embedding model. The following backends are currently supported: * SentenceTransformers * \ud83e\udd17 Transformers * Flair * Spacy * Gensim * USE (TF-Hub) You can also pass in a string that points to one of the following sentence-transformers models: * https://www.sbert.net/docs/pretrained_models.html \"\"\" self . model = select_backend ( model )","title":"__init__()"},{"location":"api/keybert.html#keybert._model.KeyBERT.extract_embeddings","text":"Extract document and word embeddings for the input documents and the generated candidate keywords/keyphrases respectively. Note that all potential keywords/keyphrases are not returned but only their word embeddings. This means that the values of candidates , keyphrase_ngram_range , stop_words , and min_df need to be the same between using .extract_embeddings and .extract_keywords . Parameters: Name Type Description Default docs Union[str, List[str]] The document(s) for which to extract keywords/keyphrases required candidates List[str] Candidate keywords/keyphrases to use instead of extracting them from the document(s) NOTE: This is not used if you passed a vectorizer . None keyphrase_ngram_range Tuple[int, int] Length, in words, of the extracted keywords/keyphrases. NOTE: This is not used if you passed a vectorizer . (1, 1) stop_words Union[str, List[str]] Stopwords to remove from the document. NOTE: This is not used if you passed a vectorizer . 'english' min_df int Minimum document frequency of a word across all documents if keywords for multiple documents need to be extracted. NOTE: This is not used if you passed a vectorizer . 1 vectorizer CountVectorizer Pass in your own CountVectorizer from sklearn.feature_extraction.text.CountVectorizer None Returns: Type Description doc_embeddings The embeddings of each document. word_embeddings: The embeddings of each potential keyword/keyphrase across across the vocabulary of the set of input documents. NOTE: The word_embeddings should be generated through .extract_embeddings as the order of these embeddings depend on the vectorizer that was used to generate its vocabulary. Usage: To generate the word and document embeddings from a set of documents: from keybert import KeyBERT kw_model = KeyBERT () doc_embeddings , word_embeddings = kw_model . extract_embeddings ( docs ) You can then use these embeddings and pass them to .extract_keywords to speed up the tuning the model: keywords = kw_model . extract_keywords ( docs , doc_embeddings = doc_embeddings , word_embeddings = word_embeddings ) Source code in keybert\\_model.py def extract_embeddings ( self , docs : Union [ str , List [ str ]], candidates : List [ str ] = None , keyphrase_ngram_range : Tuple [ int , int ] = ( 1 , 1 ), stop_words : Union [ str , List [ str ]] = \"english\" , min_df : int = 1 , vectorizer : CountVectorizer = None ) -> Union [ List [ Tuple [ str , float ]], List [ List [ Tuple [ str , float ]]]]: \"\"\"Extract document and word embeddings for the input documents and the generated candidate keywords/keyphrases respectively. Note that all potential keywords/keyphrases are not returned but only their word embeddings. This means that the values of `candidates`, `keyphrase_ngram_range`, `stop_words`, and `min_df` need to be the same between using `.extract_embeddings` and `.extract_keywords`. Arguments: docs: The document(s) for which to extract keywords/keyphrases candidates: Candidate keywords/keyphrases to use instead of extracting them from the document(s) NOTE: This is not used if you passed a `vectorizer`. keyphrase_ngram_range: Length, in words, of the extracted keywords/keyphrases. NOTE: This is not used if you passed a `vectorizer`. stop_words: Stopwords to remove from the document. NOTE: This is not used if you passed a `vectorizer`. min_df: Minimum document frequency of a word across all documents if keywords for multiple documents need to be extracted. NOTE: This is not used if you passed a `vectorizer`. vectorizer: Pass in your own `CountVectorizer` from `sklearn.feature_extraction.text.CountVectorizer` Returns: doc_embeddings: The embeddings of each document. word_embeddings: The embeddings of each potential keyword/keyphrase across across the vocabulary of the set of input documents. NOTE: The `word_embeddings` should be generated through `.extract_embeddings` as the order of these embeddings depend on the vectorizer that was used to generate its vocabulary. Usage: To generate the word and document embeddings from a set of documents: ```python from keybert import KeyBERT kw_model = KeyBERT() doc_embeddings, word_embeddings = kw_model.extract_embeddings(docs) ``` You can then use these embeddings and pass them to `.extract_keywords` to speed up the tuning the model: ```python keywords = kw_model.extract_keywords(docs, doc_embeddings=doc_embeddings, word_embeddings=word_embeddings) ``` \"\"\" # Check for a single, empty document if isinstance ( docs , str ): if docs : docs = [ docs ] else : return [] # Extract potential words using a vectorizer / tokenizer if vectorizer : count = vectorizer . fit ( docs ) else : try : count = CountVectorizer ( ngram_range = keyphrase_ngram_range , stop_words = stop_words , min_df = min_df , vocabulary = candidates , ) . fit ( docs ) except ValueError : return [] # Scikit-Learn Deprecation: get_feature_names is deprecated in 1.0 # and will be removed in 1.2. Please use get_feature_names_out instead. if version . parse ( sklearn_version ) >= version . parse ( \"1.0.0\" ): words = count . get_feature_names_out () else : words = count . get_feature_names () doc_embeddings = self . model . embed ( docs ) word_embeddings = self . model . embed ( words ) return doc_embeddings , word_embeddings","title":"extract_embeddings()"},{"location":"api/keybert.html#keybert._model.KeyBERT.extract_keywords","text":"Extract keywords and/or keyphrases To get the biggest speed-up, make sure to pass multiple documents at once instead of iterating over a single document. Parameters: Name Type Description Default docs Union[str, List[str]] The document(s) for which to extract keywords/keyphrases required candidates List[str] Candidate keywords/keyphrases to use instead of extracting them from the document(s) NOTE: This is not used if you passed a vectorizer . None keyphrase_ngram_range Tuple[int, int] Length, in words, of the extracted keywords/keyphrases. NOTE: This is not used if you passed a vectorizer . (1, 1) stop_words Union[str, List[str]] Stopwords to remove from the document. NOTE: This is not used if you passed a vectorizer . 'english' top_n int Return the top n keywords/keyphrases 5 min_df int Minimum document frequency of a word across all documents if keywords for multiple documents need to be extracted. NOTE: This is not used if you passed a vectorizer . 1 use_maxsum bool Whether to use Max Sum Distance for the selection of keywords/keyphrases. False use_mmr bool Whether to use Maximal Marginal Relevance (MMR) for the selection of keywords/keyphrases. False diversity float The diversity of the results between 0 and 1 if use_mmr is set to True. 0.5 nr_candidates int The number of candidates to consider if use_maxsum is set to True. 20 vectorizer CountVectorizer Pass in your own CountVectorizer from sklearn.feature_extraction.text.CountVectorizer None highlight bool Whether to print the document and highlight its keywords/keyphrases. NOTE: This does not work if multiple documents are passed. False seed_keywords List[str] Seed keywords that may guide the extraction of keywords by steering the similarities towards the seeded keywords. None doc_embeddings <built-in function array> The embeddings of each document. None word_embeddings <built-in function array> The embeddings of each potential keyword/keyphrase across across the vocabulary of the set of input documents. NOTE: The word_embeddings should be generated through .extract_embeddings as the order of these embeddings depend on the vectorizer that was used to generate its vocabulary. None Returns: Type Description keywords The top n keywords for a document with their respective distances to the input document. Usage: To extract keywords from a single document: from keybert import KeyBERT kw_model = KeyBERT () keywords = kw_model . extract_keywords ( doc ) To extract keywords from multiple documents, which is typically quite a bit faster: from keybert import KeyBERT kw_model = KeyBERT () keywords = kw_model . extract_keywords ( docs ) Source code in keybert\\_model.py def extract_keywords ( self , docs : Union [ str , List [ str ]], candidates : List [ str ] = None , keyphrase_ngram_range : Tuple [ int , int ] = ( 1 , 1 ), stop_words : Union [ str , List [ str ]] = \"english\" , top_n : int = 5 , min_df : int = 1 , use_maxsum : bool = False , use_mmr : bool = False , diversity : float = 0.5 , nr_candidates : int = 20 , vectorizer : CountVectorizer = None , highlight : bool = False , seed_keywords : List [ str ] = None , doc_embeddings : np . array = None , word_embeddings : np . array = None , ) -> Union [ List [ Tuple [ str , float ]], List [ List [ Tuple [ str , float ]]]]: \"\"\"Extract keywords and/or keyphrases To get the biggest speed-up, make sure to pass multiple documents at once instead of iterating over a single document. Arguments: docs: The document(s) for which to extract keywords/keyphrases candidates: Candidate keywords/keyphrases to use instead of extracting them from the document(s) NOTE: This is not used if you passed a `vectorizer`. keyphrase_ngram_range: Length, in words, of the extracted keywords/keyphrases. NOTE: This is not used if you passed a `vectorizer`. stop_words: Stopwords to remove from the document. NOTE: This is not used if you passed a `vectorizer`. top_n: Return the top n keywords/keyphrases min_df: Minimum document frequency of a word across all documents if keywords for multiple documents need to be extracted. NOTE: This is not used if you passed a `vectorizer`. use_maxsum: Whether to use Max Sum Distance for the selection of keywords/keyphrases. use_mmr: Whether to use Maximal Marginal Relevance (MMR) for the selection of keywords/keyphrases. diversity: The diversity of the results between 0 and 1 if `use_mmr` is set to True. nr_candidates: The number of candidates to consider if `use_maxsum` is set to True. vectorizer: Pass in your own `CountVectorizer` from `sklearn.feature_extraction.text.CountVectorizer` highlight: Whether to print the document and highlight its keywords/keyphrases. NOTE: This does not work if multiple documents are passed. seed_keywords: Seed keywords that may guide the extraction of keywords by steering the similarities towards the seeded keywords. doc_embeddings: The embeddings of each document. word_embeddings: The embeddings of each potential keyword/keyphrase across across the vocabulary of the set of input documents. NOTE: The `word_embeddings` should be generated through `.extract_embeddings` as the order of these embeddings depend on the vectorizer that was used to generate its vocabulary. Returns: keywords: The top n keywords for a document with their respective distances to the input document. Usage: To extract keywords from a single document: ```python from keybert import KeyBERT kw_model = KeyBERT() keywords = kw_model.extract_keywords(doc) ``` To extract keywords from multiple documents, which is typically quite a bit faster: ```python from keybert import KeyBERT kw_model = KeyBERT() keywords = kw_model.extract_keywords(docs) ``` \"\"\" # Check for a single, empty document if isinstance ( docs , str ): if docs : docs = [ docs ] else : return [] # Extract potential words using a vectorizer / tokenizer if vectorizer : count = vectorizer . fit ( docs ) else : try : count = CountVectorizer ( ngram_range = keyphrase_ngram_range , stop_words = stop_words , min_df = min_df , vocabulary = candidates , ) . fit ( docs ) except ValueError : return [] # Scikit-Learn Deprecation: get_feature_names is deprecated in 1.0 # and will be removed in 1.2. Please use get_feature_names_out instead. if version . parse ( sklearn_version ) >= version . parse ( \"1.0.0\" ): words = count . get_feature_names_out () else : words = count . get_feature_names () df = count . transform ( docs ) # Check if the right number of word embeddings are generated compared with the vectorizer if word_embeddings is not None : if word_embeddings . shape [ 0 ] != len ( words ): raise ValueError ( \"Make sure that the `word_embeddings` are generated from the function \" \"`.extract_embeddings`. \\n Moreover, the `candidates`, `keyphrase_ngram_range`,\" \"`stop_words`, and `min_df` parameters need to have the same values in both \" \"`.extract_embeddings` and `.extract_keywords`.\" ) # Extract embeddings if doc_embeddings is None : doc_embeddings = self . model . embed ( docs ) if word_embeddings is None : word_embeddings = self . model . embed ( words ) if seed_keywords is not None : seed_embeddings = self . model . embed ([ \" \" . join ( seed_keywords )]) # Find keywords all_keywords = [] for index , _ in enumerate ( docs ): try : # Select embeddings candidate_indices = df [ index ] . nonzero ()[ 1 ] candidates = [ words [ index ] for index in candidate_indices ] candidate_embeddings = word_embeddings [ candidate_indices ] doc_embedding = doc_embeddings [ index ] . reshape ( 1 , - 1 ) # Guided KeyBERT with seed keywords if seed_keywords is not None : doc_embedding = np . average ( [ doc_embedding , seed_embeddings ], axis = 0 , weights = [ 3 , 1 ] ) # Maximal Marginal Relevance (MMR) if use_mmr : keywords = mmr ( doc_embedding , candidate_embeddings , candidates , top_n , diversity , ) # Max Sum Distance elif use_maxsum : keywords = max_sum_distance ( doc_embedding , candidate_embeddings , candidates , top_n , nr_candidates , ) # Cosine-based keyword extraction else : distances = cosine_similarity ( doc_embedding , candidate_embeddings ) keywords = [ ( candidates [ index ], round ( float ( distances [ 0 ][ index ]), 4 )) for index in distances . argsort ()[ 0 ][ - top_n :] ][:: - 1 ] all_keywords . append ( keywords ) # Capturing empty keywords except ValueError : all_keywords . append ([]) # Highlight keywords in the document if len ( all_keywords ) == 1 : if highlight : highlight_document ( docs [ 0 ], all_keywords [ 0 ], count ) all_keywords = all_keywords [ 0 ] return all_keywords","title":"extract_keywords()"},{"location":"api/maxsum.html","text":"Max Sum Distance \u00b6 Calculate Max Sum Distance for extraction of keywords We take the 2 x top_n most similar words/phrases to the document. Then, we take all top_n combinations from the 2 x top_n words and extract the combination that are the least similar to each other by cosine similarity. This is O(n^2) and therefore not advised if you use a large top_n Parameters: Name Type Description Default doc_embedding ndarray The document embeddings required word_embeddings ndarray The embeddings of the selected candidate keywords/phrases required words List[str] The selected candidate keywords/keyphrases required top_n int The number of keywords/keyhprases to return required nr_candidates int The number of candidates to consider required Returns: Type Description List[Tuple[str, float]] The selected keywords/keyphrases with their distances Source code in keybert\\_maxsum.py def max_sum_distance ( doc_embedding : np . ndarray , word_embeddings : np . ndarray , words : List [ str ], top_n : int , nr_candidates : int , ) -> List [ Tuple [ str , float ]]: \"\"\"Calculate Max Sum Distance for extraction of keywords We take the 2 x top_n most similar words/phrases to the document. Then, we take all top_n combinations from the 2 x top_n words and extract the combination that are the least similar to each other by cosine similarity. This is O(n^2) and therefore not advised if you use a large `top_n` Arguments: doc_embedding: The document embeddings word_embeddings: The embeddings of the selected candidate keywords/phrases words: The selected candidate keywords/keyphrases top_n: The number of keywords/keyhprases to return nr_candidates: The number of candidates to consider Returns: List[Tuple[str, float]]: The selected keywords/keyphrases with their distances \"\"\" if nr_candidates < top_n : raise Exception ( \"Make sure that the number of candidates exceeds the number \" \"of keywords to return.\" ) elif top_n > len ( words ): return [] # Calculate distances and extract keywords distances = cosine_similarity ( doc_embedding , word_embeddings ) distances_words = cosine_similarity ( word_embeddings , word_embeddings ) # Get 2*top_n words as candidates based on cosine similarity words_idx = list ( distances . argsort ()[ 0 ][ - nr_candidates :]) words_vals = [ words [ index ] for index in words_idx ] candidates = distances_words [ np . ix_ ( words_idx , words_idx )] # Calculate the combination of words that are the least similar to each other min_sim = 100_000 candidate = None for combination in itertools . combinations ( range ( len ( words_idx )), top_n ): sim = sum ( [ candidates [ i ][ j ] for i in combination for j in combination if i != j ] ) if sim < min_sim : candidate = combination min_sim = sim return [ ( words_vals [ idx ], round ( float ( distances [ 0 ][ words_idx [ idx ]]), 4 )) for idx in candidate ]","title":"MaxSum"},{"location":"api/maxsum.html#max-sum-distance","text":"Calculate Max Sum Distance for extraction of keywords We take the 2 x top_n most similar words/phrases to the document. Then, we take all top_n combinations from the 2 x top_n words and extract the combination that are the least similar to each other by cosine similarity. This is O(n^2) and therefore not advised if you use a large top_n Parameters: Name Type Description Default doc_embedding ndarray The document embeddings required word_embeddings ndarray The embeddings of the selected candidate keywords/phrases required words List[str] The selected candidate keywords/keyphrases required top_n int The number of keywords/keyhprases to return required nr_candidates int The number of candidates to consider required Returns: Type Description List[Tuple[str, float]] The selected keywords/keyphrases with their distances Source code in keybert\\_maxsum.py def max_sum_distance ( doc_embedding : np . ndarray , word_embeddings : np . ndarray , words : List [ str ], top_n : int , nr_candidates : int , ) -> List [ Tuple [ str , float ]]: \"\"\"Calculate Max Sum Distance for extraction of keywords We take the 2 x top_n most similar words/phrases to the document. Then, we take all top_n combinations from the 2 x top_n words and extract the combination that are the least similar to each other by cosine similarity. This is O(n^2) and therefore not advised if you use a large `top_n` Arguments: doc_embedding: The document embeddings word_embeddings: The embeddings of the selected candidate keywords/phrases words: The selected candidate keywords/keyphrases top_n: The number of keywords/keyhprases to return nr_candidates: The number of candidates to consider Returns: List[Tuple[str, float]]: The selected keywords/keyphrases with their distances \"\"\" if nr_candidates < top_n : raise Exception ( \"Make sure that the number of candidates exceeds the number \" \"of keywords to return.\" ) elif top_n > len ( words ): return [] # Calculate distances and extract keywords distances = cosine_similarity ( doc_embedding , word_embeddings ) distances_words = cosine_similarity ( word_embeddings , word_embeddings ) # Get 2*top_n words as candidates based on cosine similarity words_idx = list ( distances . argsort ()[ 0 ][ - nr_candidates :]) words_vals = [ words [ index ] for index in words_idx ] candidates = distances_words [ np . ix_ ( words_idx , words_idx )] # Calculate the combination of words that are the least similar to each other min_sim = 100_000 candidate = None for combination in itertools . combinations ( range ( len ( words_idx )), top_n ): sim = sum ( [ candidates [ i ][ j ] for i in combination for j in combination if i != j ] ) if sim < min_sim : candidate = combination min_sim = sim return [ ( words_vals [ idx ], round ( float ( distances [ 0 ][ words_idx [ idx ]]), 4 )) for idx in candidate ]","title":"Max Sum Distance"},{"location":"api/mmr.html","text":"Maximal Marginal Relevance \u00b6 Calculate Maximal Marginal Relevance (MMR) between candidate keywords and the document. MMR considers the similarity of keywords/keyphrases with the document, along with the similarity of already selected keywords and keyphrases. This results in a selection of keywords that maximize their within diversity with respect to the document. Parameters: Name Type Description Default doc_embedding ndarray The document embeddings required word_embeddings ndarray The embeddings of the selected candidate keywords/phrases required words List[str] The selected candidate keywords/keyphrases required top_n int The number of keywords/keyhprases to return 5 diversity float How diverse the select keywords/keyphrases are. Values between 0 and 1 with 0 being not diverse at all and 1 being most diverse. 0.8 Returns: Type Description List[Tuple[str, float]] The selected keywords/keyphrases with their distances Source code in keybert\\_mmr.py def mmr ( doc_embedding : np . ndarray , word_embeddings : np . ndarray , words : List [ str ], top_n : int = 5 , diversity : float = 0.8 , ) -> List [ Tuple [ str , float ]]: \"\"\"Calculate Maximal Marginal Relevance (MMR) between candidate keywords and the document. MMR considers the similarity of keywords/keyphrases with the document, along with the similarity of already selected keywords and keyphrases. This results in a selection of keywords that maximize their within diversity with respect to the document. Arguments: doc_embedding: The document embeddings word_embeddings: The embeddings of the selected candidate keywords/phrases words: The selected candidate keywords/keyphrases top_n: The number of keywords/keyhprases to return diversity: How diverse the select keywords/keyphrases are. Values between 0 and 1 with 0 being not diverse at all and 1 being most diverse. Returns: List[Tuple[str, float]]: The selected keywords/keyphrases with their distances \"\"\" # Extract similarity within words, and between words and the document word_doc_similarity = cosine_similarity ( word_embeddings , doc_embedding ) word_similarity = cosine_similarity ( word_embeddings ) # Initialize candidates and already choose best keyword/keyphras keywords_idx = [ np . argmax ( word_doc_similarity )] candidates_idx = [ i for i in range ( len ( words )) if i != keywords_idx [ 0 ]] for _ in range ( min ( top_n - 1 , len ( words ) - 1 )): # Extract similarities within candidates and # between candidates and selected keywords/phrases candidate_similarities = word_doc_similarity [ candidates_idx , :] target_similarities = np . max ( word_similarity [ candidates_idx ][:, keywords_idx ], axis = 1 ) # Calculate MMR mmr = ( 1 - diversity ) * candidate_similarities - diversity * target_similarities . reshape ( - 1 , 1 ) mmr_idx = candidates_idx [ np . argmax ( mmr )] # Update keywords & candidates keywords_idx . append ( mmr_idx ) candidates_idx . remove ( mmr_idx ) # Extract and sort keywords in descending similarity keywords = [ ( words [ idx ], round ( float ( word_doc_similarity . reshape ( 1 , - 1 )[ 0 ][ idx ]), 4 )) for idx in keywords_idx ] keywords = sorted ( keywords , key = itemgetter ( 1 ), reverse = True ) return keywords","title":"MMR"},{"location":"api/mmr.html#maximal-marginal-relevance","text":"Calculate Maximal Marginal Relevance (MMR) between candidate keywords and the document. MMR considers the similarity of keywords/keyphrases with the document, along with the similarity of already selected keywords and keyphrases. This results in a selection of keywords that maximize their within diversity with respect to the document. Parameters: Name Type Description Default doc_embedding ndarray The document embeddings required word_embeddings ndarray The embeddings of the selected candidate keywords/phrases required words List[str] The selected candidate keywords/keyphrases required top_n int The number of keywords/keyhprases to return 5 diversity float How diverse the select keywords/keyphrases are. Values between 0 and 1 with 0 being not diverse at all and 1 being most diverse. 0.8 Returns: Type Description List[Tuple[str, float]] The selected keywords/keyphrases with their distances Source code in keybert\\_mmr.py def mmr ( doc_embedding : np . ndarray , word_embeddings : np . ndarray , words : List [ str ], top_n : int = 5 , diversity : float = 0.8 , ) -> List [ Tuple [ str , float ]]: \"\"\"Calculate Maximal Marginal Relevance (MMR) between candidate keywords and the document. MMR considers the similarity of keywords/keyphrases with the document, along with the similarity of already selected keywords and keyphrases. This results in a selection of keywords that maximize their within diversity with respect to the document. Arguments: doc_embedding: The document embeddings word_embeddings: The embeddings of the selected candidate keywords/phrases words: The selected candidate keywords/keyphrases top_n: The number of keywords/keyhprases to return diversity: How diverse the select keywords/keyphrases are. Values between 0 and 1 with 0 being not diverse at all and 1 being most diverse. Returns: List[Tuple[str, float]]: The selected keywords/keyphrases with their distances \"\"\" # Extract similarity within words, and between words and the document word_doc_similarity = cosine_similarity ( word_embeddings , doc_embedding ) word_similarity = cosine_similarity ( word_embeddings ) # Initialize candidates and already choose best keyword/keyphras keywords_idx = [ np . argmax ( word_doc_similarity )] candidates_idx = [ i for i in range ( len ( words )) if i != keywords_idx [ 0 ]] for _ in range ( min ( top_n - 1 , len ( words ) - 1 )): # Extract similarities within candidates and # between candidates and selected keywords/phrases candidate_similarities = word_doc_similarity [ candidates_idx , :] target_similarities = np . max ( word_similarity [ candidates_idx ][:, keywords_idx ], axis = 1 ) # Calculate MMR mmr = ( 1 - diversity ) * candidate_similarities - diversity * target_similarities . reshape ( - 1 , 1 ) mmr_idx = candidates_idx [ np . argmax ( mmr )] # Update keywords & candidates keywords_idx . append ( mmr_idx ) candidates_idx . remove ( mmr_idx ) # Extract and sort keywords in descending similarity keywords = [ ( words [ idx ], round ( float ( word_doc_similarity . reshape ( 1 , - 1 )[ 0 ][ idx ]), 4 )) for idx in keywords_idx ] keywords = sorted ( keywords , key = itemgetter ( 1 ), reverse = True ) return keywords","title":"Maximal Marginal Relevance"},{"location":"guides/countvectorizer.html","text":"CountVectorizer Tips & Tricks \u00b6 An unexpectly important component of KeyBERT is the CountVectorizer. In KeyBERT, it is used to split up your documents into candidate keywords and keyphrases. However, there is much more flexibility with the CountVectorizer than you might have initially thought. Since we use the vectorizer to split up the documents after embedding them, we can parse the document however we want as it does not affect the quality of the document embeddings. In this page, we will go through several examples of how you can take the CountVectorizer to the next level and improve upon the generated keywords. Basic Usage \u00b6 First, let's start with defining our text and the keyword model: from keybert import KeyBERT doc = \"\"\" Supervised learning is the machine learning task of learning a function that maps an input to an output based on example input-output pairs.[1] It infers a function from labeled training data consisting of a set of training examples.[2] In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a 'reasonable' way (see inductive bias). \"\"\" kw_model = KeyBERT () We will use the above code throughout this tutorial as the base and built upon it with the CountVectorizer. Next, we can use a basic vectorizer when extracting keywords as follows: >>> vectorizer = CountVectorizer () >>> keywords = kw_model . extract_keywords ( doc , vectorizer = vectorizer ) [( 'learning' , 0.4604 ), ( 'algorithm' , 0.4556 ), ( 'training' , 0.4487 ), ( 'class' , 0.4086 ), ( 'mapping' , 0.3700 )] NOTE Although I typically like to use use_mmr=True as it often improves upon the generated keywords, this tutorial will do without in order give you a clear view of the effects of the CountVectorizer. Parameters \u00b6 There are a number of basic parameters in the CountVectorizer that we can use to improve upon the quality of the resulting keywords. ngram_range \u00b6 By setting the ngram_range we can decide how many tokens the keyphrases needs to be as a minimum and how long it can be as a maximum: >>> vectorizer = CountVectorizer ( ngram_range = ( 1 , 3 )) >>> kw_model . extract_keywords ( doc , vectorizer = vectorizer ) [( 'supervised learning is' , 0.7048 ), ( 'supervised learning algorithm' , 0.6834 ), ( 'supervised learning' , 0.6658 ), ( 'supervised' , 0.6523 ), ( 'in supervised learning' , 0.6474 )] As we can see, the length of the resulting keyphrases are higher than what we have seen before. This may happen as embeddings in vector space are often closer in distance if their document counterparts in similar in size. There are two interesting things happening here. First, there are many similar keyphrases that we want to diversify, which we can achieve by setting use_mmr=True . Second, you may have noticed stopwords appearing in the keyphrases. That we can solve by following the section below! stop_words \u00b6 As we have seen in the results above, stopwords might appear in your keyphrases. To remove them, we can tell the CountVectorizer to either remove a list of keywords that we supplied ourselves or simply state for which language stopwords need to be removed: >>> vectorizer = CountVectorizer ( ngram_range = ( 1 , 3 ), stop_words = \"english\" ) >>> kw_model . extract_keywords ( doc , vectorizer = vectorizer ) [( 'supervised learning algorithm' , 0.6834 ), ( 'supervised learning' , 0.6658 ), ( 'supervised learning example' , 0.6641 ), ( 'supervised learning machine' , 0.6528 ), ( 'function labeled training' , 0.6526 )] This already looks much better! The stopwords are removed and the resulting keyphrases already look a bit more interesting and useful. vocabulary \u00b6 For some use cases, keywords can only be generated from predefined vocabularies. For example, when you already have a list of possible keywords you can use those as a vocabulary and ask the CountVectorizer to only select keywords from that list. First, let's define our vocabulary: vocab = [ 'produces inferred function' , 'supervised' , 'inductive' , 'function' , 'bias' , 'supervisory' , 'supervised learning' , 'infers function' , 'supervisory signal' , 'inductive bias' , 'unseen instances' ] Then, we pass that vocabulary to our CountVectorizer and extract our keywords: >>> vectorizer = CountVectorizer ( ngram_range = ( 1 , 3 ), vocabulary = vocab ) >>> kw_model . extract_keywords ( doc , vectorizer = vectorizer ) [( 'supervised learning' , 0.6658 ), ( 'supervised' , 0.6523 ), ( 'supervisory signal' , 0.357 ), ( 'inductive bias' , 0.3377 ), ( 'produces inferred function' , 0.3365 )] tokenizer \u00b6 The default tokenizer in the CountVectorizer works well for western languages but fails to tokenize some non-western languages, like Chinese. Fortunately, we can use the tokenizer variable in the CountVectorizer to use jieba , which is a package for Chinese text segmentation. Using it is straightforward: from sklearn.feature_extraction.text import CountVectorizer import jieba def tokenize_zh ( text ): words = jieba . lcut ( text ) return words vectorizer = CountVectorizer ( tokenizer = tokenize_zh ) Then, simply pass the vectorizer to your KeyBERT instance: from keybert import KeyBERT kw_model = KeyBERT () keywords = kw_model . extract_keywords ( doc , vectorizer = vectorizer ) KeyphraseVectorizers \u00b6 To even further enhance the possibilities of the CountVectorizer, Tim Schopf created an excellent package, KeyphraseVectorizers , that enriches the CountVectorizer with the possibilities to extract keyphrases with part-of-speech patterns using the Spacy library. The great thing about the KeyphraseVectorizers is that aside from leveraging part-of-speech patterns, it automatically extract keyphrases without the need to specify an n-gram range. That by itself is an amazing feature to have! Other advantages of this package: Extract grammatically accurate keyphases based on their part-of-speech tags. No need to specify n-gram ranges. Get document-keyphrase matrices. Multiple language support. User-defined part-of-speech patterns for keyphrase extraction possible. Usage \u00b6 First, we need to install the package: pip install keyphrase-vectorizers Then, let's see what the output looks like with the basic CountVectorizer using a larger n-gram value: >>> vectorizer = CountVectorizer ( ngram_range = ( 1 , 3 )) >>> kw_model . extract_keywords ( doc , vectorizer = vectorizer ) [( 'supervised learning is' , 0.7048 ), ( 'supervised learning algorithm' , 0.6834 ), ( 'supervised learning' , 0.6658 ), ( 'supervised' , 0.6523 ), ( 'in supervised learning' , 0.6474 )] Not bad but as we have seen before, this can definitely be improved. Now, let's use the KeyphraseCountVectorizer instead: >>> from keyphrase_vectorizers import KeyphraseCountVectorizer >>> vectorizer = KeyphraseCountVectorizer () >>> kw_model . extract_keywords ( doc , vectorizer = vectorizer ) [( 'supervised learning algorithm' , 0.6834 ), ( 'supervised learning' , 0.6658 ), ( 'learning algorithm' , 0.5549 ), ( 'training data' , 0.511 ), ( 'training' , 0.3858 )] A large improvement compared to the basic CountVectorizer! Now, as seen before it seems that there are still some repeated keyphrases that we want to remove. To do this, we again leverage the MMR function on top of KeyBERT to diversify the output: >>> from keyphrase_vectorizers import KeyphraseCountVectorizer >>> vectorizer = KeyphraseCountVectorizer () >>> kw_model . extract_keywords ( doc , vectorizer = vectorizer , use_mmr = True ) [( 'supervised learning algorithm' , 0.6834 ), ( 'unseen instances' , 0.3246 ), ( 'supervisory signal' , 0.357 ), ( 'inductive bias' , 0.3377 ), ( 'class labels' , 0.3715 )] We can see much more diverse keyphrases and based on the input document the keyphrases also make sense. Languages \u00b6 Those familiar with Spacy might know that in order to use part-of-speech, we need a language-specific model. You can find an overview of these models here . To change the language model, we only need to change one parameter in order to select a different language: vectorizer = KeyphraseCountVectorizer ( spacy_pipeline = 'de_core_news_sm' ) Part-of-speech \u00b6 KeyphraseVectorizers extracts the part-of-speech tags from the documents and then applies a regex pattern to extract keyphrases that fit within that pattern. The default pattern is <J.*>*<N.*>+ which means that it extract keyphrases that have 0 or more adjectives followed by 1 or more nouns. However, we might not agree with that for our specific use case! Fortunately, the package allows you to use a different pattern. To visualize the effect, let's first perform it with the default settings: >>> vectorizer = KeyphraseCountVectorizer () >>> kw_model . extract_keywords ( doc , vectorizer = vectorizer ) [( 'supervised learning algorithm' , 0.6834 ), ( 'supervised learning' , 0.6658 ), ( 'learning algorithm' , 0.5549 ), ( 'training data' , 0.511 ), ( 'training' , 0.3858 )] Although the above keyphrases seem accurate, we might want to only extract a noun from the documents in order to only extract keywords and not keyphrases: >>> vectorizer = KeyphraseCountVectorizer ( pos_pattern = '<N.*>' ) >>> kw_model . extract_keywords ( doc , vectorizer = vectorizer ) [( 'learning' , 0.467 ), ( 'training' , 0.3858 ), ( 'labels' , 0.3728 ), ( 'data' , 0.2993 ), ( 'algorithm' , 0.2827 )] These seem much better as keywords now that we focus only on nouns in the document.","title":"CountVectorizer"},{"location":"guides/countvectorizer.html#countvectorizer-tips-tricks","text":"An unexpectly important component of KeyBERT is the CountVectorizer. In KeyBERT, it is used to split up your documents into candidate keywords and keyphrases. However, there is much more flexibility with the CountVectorizer than you might have initially thought. Since we use the vectorizer to split up the documents after embedding them, we can parse the document however we want as it does not affect the quality of the document embeddings. In this page, we will go through several examples of how you can take the CountVectorizer to the next level and improve upon the generated keywords.","title":"CountVectorizer Tips &amp; Tricks"},{"location":"guides/countvectorizer.html#basic-usage","text":"First, let's start with defining our text and the keyword model: from keybert import KeyBERT doc = \"\"\" Supervised learning is the machine learning task of learning a function that maps an input to an output based on example input-output pairs.[1] It infers a function from labeled training data consisting of a set of training examples.[2] In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a 'reasonable' way (see inductive bias). \"\"\" kw_model = KeyBERT () We will use the above code throughout this tutorial as the base and built upon it with the CountVectorizer. Next, we can use a basic vectorizer when extracting keywords as follows: >>> vectorizer = CountVectorizer () >>> keywords = kw_model . extract_keywords ( doc , vectorizer = vectorizer ) [( 'learning' , 0.4604 ), ( 'algorithm' , 0.4556 ), ( 'training' , 0.4487 ), ( 'class' , 0.4086 ), ( 'mapping' , 0.3700 )] NOTE Although I typically like to use use_mmr=True as it often improves upon the generated keywords, this tutorial will do without in order give you a clear view of the effects of the CountVectorizer.","title":"Basic Usage"},{"location":"guides/countvectorizer.html#parameters","text":"There are a number of basic parameters in the CountVectorizer that we can use to improve upon the quality of the resulting keywords.","title":"Parameters"},{"location":"guides/countvectorizer.html#ngram_range","text":"By setting the ngram_range we can decide how many tokens the keyphrases needs to be as a minimum and how long it can be as a maximum: >>> vectorizer = CountVectorizer ( ngram_range = ( 1 , 3 )) >>> kw_model . extract_keywords ( doc , vectorizer = vectorizer ) [( 'supervised learning is' , 0.7048 ), ( 'supervised learning algorithm' , 0.6834 ), ( 'supervised learning' , 0.6658 ), ( 'supervised' , 0.6523 ), ( 'in supervised learning' , 0.6474 )] As we can see, the length of the resulting keyphrases are higher than what we have seen before. This may happen as embeddings in vector space are often closer in distance if their document counterparts in similar in size. There are two interesting things happening here. First, there are many similar keyphrases that we want to diversify, which we can achieve by setting use_mmr=True . Second, you may have noticed stopwords appearing in the keyphrases. That we can solve by following the section below!","title":"ngram_range"},{"location":"guides/countvectorizer.html#stop_words","text":"As we have seen in the results above, stopwords might appear in your keyphrases. To remove them, we can tell the CountVectorizer to either remove a list of keywords that we supplied ourselves or simply state for which language stopwords need to be removed: >>> vectorizer = CountVectorizer ( ngram_range = ( 1 , 3 ), stop_words = \"english\" ) >>> kw_model . extract_keywords ( doc , vectorizer = vectorizer ) [( 'supervised learning algorithm' , 0.6834 ), ( 'supervised learning' , 0.6658 ), ( 'supervised learning example' , 0.6641 ), ( 'supervised learning machine' , 0.6528 ), ( 'function labeled training' , 0.6526 )] This already looks much better! The stopwords are removed and the resulting keyphrases already look a bit more interesting and useful.","title":"stop_words"},{"location":"guides/countvectorizer.html#vocabulary","text":"For some use cases, keywords can only be generated from predefined vocabularies. For example, when you already have a list of possible keywords you can use those as a vocabulary and ask the CountVectorizer to only select keywords from that list. First, let's define our vocabulary: vocab = [ 'produces inferred function' , 'supervised' , 'inductive' , 'function' , 'bias' , 'supervisory' , 'supervised learning' , 'infers function' , 'supervisory signal' , 'inductive bias' , 'unseen instances' ] Then, we pass that vocabulary to our CountVectorizer and extract our keywords: >>> vectorizer = CountVectorizer ( ngram_range = ( 1 , 3 ), vocabulary = vocab ) >>> kw_model . extract_keywords ( doc , vectorizer = vectorizer ) [( 'supervised learning' , 0.6658 ), ( 'supervised' , 0.6523 ), ( 'supervisory signal' , 0.357 ), ( 'inductive bias' , 0.3377 ), ( 'produces inferred function' , 0.3365 )]","title":"vocabulary"},{"location":"guides/countvectorizer.html#tokenizer","text":"The default tokenizer in the CountVectorizer works well for western languages but fails to tokenize some non-western languages, like Chinese. Fortunately, we can use the tokenizer variable in the CountVectorizer to use jieba , which is a package for Chinese text segmentation. Using it is straightforward: from sklearn.feature_extraction.text import CountVectorizer import jieba def tokenize_zh ( text ): words = jieba . lcut ( text ) return words vectorizer = CountVectorizer ( tokenizer = tokenize_zh ) Then, simply pass the vectorizer to your KeyBERT instance: from keybert import KeyBERT kw_model = KeyBERT () keywords = kw_model . extract_keywords ( doc , vectorizer = vectorizer )","title":"tokenizer"},{"location":"guides/countvectorizer.html#keyphrasevectorizers","text":"To even further enhance the possibilities of the CountVectorizer, Tim Schopf created an excellent package, KeyphraseVectorizers , that enriches the CountVectorizer with the possibilities to extract keyphrases with part-of-speech patterns using the Spacy library. The great thing about the KeyphraseVectorizers is that aside from leveraging part-of-speech patterns, it automatically extract keyphrases without the need to specify an n-gram range. That by itself is an amazing feature to have! Other advantages of this package: Extract grammatically accurate keyphases based on their part-of-speech tags. No need to specify n-gram ranges. Get document-keyphrase matrices. Multiple language support. User-defined part-of-speech patterns for keyphrase extraction possible.","title":"KeyphraseVectorizers"},{"location":"guides/countvectorizer.html#usage","text":"First, we need to install the package: pip install keyphrase-vectorizers Then, let's see what the output looks like with the basic CountVectorizer using a larger n-gram value: >>> vectorizer = CountVectorizer ( ngram_range = ( 1 , 3 )) >>> kw_model . extract_keywords ( doc , vectorizer = vectorizer ) [( 'supervised learning is' , 0.7048 ), ( 'supervised learning algorithm' , 0.6834 ), ( 'supervised learning' , 0.6658 ), ( 'supervised' , 0.6523 ), ( 'in supervised learning' , 0.6474 )] Not bad but as we have seen before, this can definitely be improved. Now, let's use the KeyphraseCountVectorizer instead: >>> from keyphrase_vectorizers import KeyphraseCountVectorizer >>> vectorizer = KeyphraseCountVectorizer () >>> kw_model . extract_keywords ( doc , vectorizer = vectorizer ) [( 'supervised learning algorithm' , 0.6834 ), ( 'supervised learning' , 0.6658 ), ( 'learning algorithm' , 0.5549 ), ( 'training data' , 0.511 ), ( 'training' , 0.3858 )] A large improvement compared to the basic CountVectorizer! Now, as seen before it seems that there are still some repeated keyphrases that we want to remove. To do this, we again leverage the MMR function on top of KeyBERT to diversify the output: >>> from keyphrase_vectorizers import KeyphraseCountVectorizer >>> vectorizer = KeyphraseCountVectorizer () >>> kw_model . extract_keywords ( doc , vectorizer = vectorizer , use_mmr = True ) [( 'supervised learning algorithm' , 0.6834 ), ( 'unseen instances' , 0.3246 ), ( 'supervisory signal' , 0.357 ), ( 'inductive bias' , 0.3377 ), ( 'class labels' , 0.3715 )] We can see much more diverse keyphrases and based on the input document the keyphrases also make sense.","title":"Usage"},{"location":"guides/countvectorizer.html#languages","text":"Those familiar with Spacy might know that in order to use part-of-speech, we need a language-specific model. You can find an overview of these models here . To change the language model, we only need to change one parameter in order to select a different language: vectorizer = KeyphraseCountVectorizer ( spacy_pipeline = 'de_core_news_sm' )","title":"Languages"},{"location":"guides/countvectorizer.html#part-of-speech","text":"KeyphraseVectorizers extracts the part-of-speech tags from the documents and then applies a regex pattern to extract keyphrases that fit within that pattern. The default pattern is <J.*>*<N.*>+ which means that it extract keyphrases that have 0 or more adjectives followed by 1 or more nouns. However, we might not agree with that for our specific use case! Fortunately, the package allows you to use a different pattern. To visualize the effect, let's first perform it with the default settings: >>> vectorizer = KeyphraseCountVectorizer () >>> kw_model . extract_keywords ( doc , vectorizer = vectorizer ) [( 'supervised learning algorithm' , 0.6834 ), ( 'supervised learning' , 0.6658 ), ( 'learning algorithm' , 0.5549 ), ( 'training data' , 0.511 ), ( 'training' , 0.3858 )] Although the above keyphrases seem accurate, we might want to only extract a noun from the documents in order to only extract keywords and not keyphrases: >>> vectorizer = KeyphraseCountVectorizer ( pos_pattern = '<N.*>' ) >>> kw_model . extract_keywords ( doc , vectorizer = vectorizer ) [( 'learning' , 0.467 ), ( 'training' , 0.3858 ), ( 'labels' , 0.3728 ), ( 'data' , 0.2993 ), ( 'algorithm' , 0.2827 )] These seem much better as keywords now that we focus only on nouns in the document.","title":"Part-of-speech"},{"location":"guides/embeddings.html","text":"Embedding Models \u00b6 In this tutorial we will be going through the embedding models that can be used in KeyBERT. Having the option to choose embedding models allow you to leverage pre-trained embeddings that suit your use-case. Sentence Transformers \u00b6 You can select any model from sentence-transformers here and pass it through KeyBERT with model : from keybert import KeyBERT kw_model = KeyBERT ( model = \"all-MiniLM-L6-v2\" ) Or select a SentenceTransformer model with your own parameters: from sentence_transformers import SentenceTransformer sentence_model = SentenceTransformer ( \"all-MiniLM-L6-v2\" ) kw_model = KeyBERT ( model = sentence_model ) \ud83e\udd17 Hugging Face Transformers \u00b6 To use a Hugging Face transformers model, load in a pipeline and point to any model found on their model hub (https://huggingface.co/models): from transformers.pipelines import pipeline hf_model = pipeline ( \"feature-extraction\" , model = \"distilbert-base-cased\" ) kw_model = KeyBERT ( model = hf_model ) Tip! These transformers also work quite well using sentence-transformers which has a number of optimizations tricks that make using it a bit faster. Flair \u00b6 Flair allows you to choose almost any embedding model that is publicly available. Flair can be used as follows: from flair.embeddings import TransformerDocumentEmbeddings roberta = TransformerDocumentEmbeddings ( 'roberta-base' ) kw_model = KeyBERT ( model = roberta ) You can select any \ud83e\udd17 transformers model here . Moreover, you can also use Flair to use word embeddings and pool them to create document embeddings. Under the hood, Flair simply averages all word embeddings in a document. Then, we can easily pass it to KeyBERT in order to use those word embeddings as document embeddings: from flair.embeddings import WordEmbeddings , DocumentPoolEmbeddings glove_embedding = WordEmbeddings ( 'crawl' ) document_glove_embeddings = DocumentPoolEmbeddings ([ glove_embedding ]) kw_model = KeyBERT ( model = document_glove_embeddings ) Spacy \u00b6 Spacy is an amazing framework for processing text. There are many models available across many languages for modeling text. To use Spacy's non-transformer models in KeyBERT: import spacy nlp = spacy . load ( \"en_core_web_md\" , exclude = [ 'tagger' , 'parser' , 'ner' , 'attribute_ruler' , 'lemmatizer' ]) kw_model = KeyBERT ( model = nlp ) Using spacy-transformer models: import spacy spacy . prefer_gpu () nlp = spacy . load ( \"en_core_web_trf\" , exclude = [ 'tagger' , 'parser' , 'ner' , 'attribute_ruler' , 'lemmatizer' ]) kw_model = KeyBERT ( model = nlp ) If you run into memory issues with spacy-transformer models, try: import spacy from thinc.api import set_gpu_allocator , require_gpu nlp = spacy . load ( \"en_core_web_trf\" , exclude = [ 'tagger' , 'parser' , 'ner' , 'attribute_ruler' , 'lemmatizer' ]) set_gpu_allocator ( \"pytorch\" ) require_gpu ( 0 ) kw_model = KeyBERT ( model = nlp ) Universal Sentence Encoder (USE) \u00b6 The Universal Sentence Encoder encodes text into high dimensional vectors that are used here for embedding the documents. The model is trained and optimized for greater-than-word length text, such as sentences, phrases or short paragraphs. Using USE in KeyBERT is rather straightforward: import tensorflow_hub embedding_model = tensorflow_hub . load ( \"https://tfhub.dev/google/universal-sentence-encoder/4\" ) kw_model = KeyBERT ( model = embedding_model ) Gensim \u00b6 For Gensim, KeyBERT supports its gensim.downloader module. Here, we can download any model word embedding model to be used in KeyBERT. Note that Gensim is primarily used for Word Embedding models. This works typically best for short documents since the word embeddings are pooled. import gensim.downloader as api ft = api . load ( 'fasttext-wiki-news-subwords-300' ) kw_model = KeyBERT ( model = ft ) Custom Backend \u00b6 If your backend or model cannot be found in the ones currently available, you can use the keybert.backend.BaseEmbedder class to create your own backend. Below, you will find an example of creating a SentenceTransformer backend for KeyBERT: from keybert.backend import BaseEmbedder from sentence_transformers import SentenceTransformer class CustomEmbedder ( BaseEmbedder ): def __init__ ( self , embedding_model ): super () . __init__ () self . embedding_model = embedding_model def embed ( self , documents , verbose = False ): embeddings = self . embedding_model . encode ( documents , show_progress_bar = verbose ) return embeddings # Create custom backend distilbert = SentenceTransformer ( \"paraphrase-MiniLM-L6-v2\" ) custom_embedder = CustomEmbedder ( embedding_model = distilbert ) # Pass custom backend to keybert kw_model = KeyBERT ( model = custom_embedder )","title":"Embedding Models"},{"location":"guides/embeddings.html#embedding-models","text":"In this tutorial we will be going through the embedding models that can be used in KeyBERT. Having the option to choose embedding models allow you to leverage pre-trained embeddings that suit your use-case.","title":"Embedding Models"},{"location":"guides/embeddings.html#sentence-transformers","text":"You can select any model from sentence-transformers here and pass it through KeyBERT with model : from keybert import KeyBERT kw_model = KeyBERT ( model = \"all-MiniLM-L6-v2\" ) Or select a SentenceTransformer model with your own parameters: from sentence_transformers import SentenceTransformer sentence_model = SentenceTransformer ( \"all-MiniLM-L6-v2\" ) kw_model = KeyBERT ( model = sentence_model )","title":"Sentence Transformers"},{"location":"guides/embeddings.html#hugging-face-transformers","text":"To use a Hugging Face transformers model, load in a pipeline and point to any model found on their model hub (https://huggingface.co/models): from transformers.pipelines import pipeline hf_model = pipeline ( \"feature-extraction\" , model = \"distilbert-base-cased\" ) kw_model = KeyBERT ( model = hf_model ) Tip! These transformers also work quite well using sentence-transformers which has a number of optimizations tricks that make using it a bit faster.","title":"\ud83e\udd17 Hugging Face Transformers"},{"location":"guides/embeddings.html#flair","text":"Flair allows you to choose almost any embedding model that is publicly available. Flair can be used as follows: from flair.embeddings import TransformerDocumentEmbeddings roberta = TransformerDocumentEmbeddings ( 'roberta-base' ) kw_model = KeyBERT ( model = roberta ) You can select any \ud83e\udd17 transformers model here . Moreover, you can also use Flair to use word embeddings and pool them to create document embeddings. Under the hood, Flair simply averages all word embeddings in a document. Then, we can easily pass it to KeyBERT in order to use those word embeddings as document embeddings: from flair.embeddings import WordEmbeddings , DocumentPoolEmbeddings glove_embedding = WordEmbeddings ( 'crawl' ) document_glove_embeddings = DocumentPoolEmbeddings ([ glove_embedding ]) kw_model = KeyBERT ( model = document_glove_embeddings )","title":"Flair"},{"location":"guides/embeddings.html#spacy","text":"Spacy is an amazing framework for processing text. There are many models available across many languages for modeling text. To use Spacy's non-transformer models in KeyBERT: import spacy nlp = spacy . load ( \"en_core_web_md\" , exclude = [ 'tagger' , 'parser' , 'ner' , 'attribute_ruler' , 'lemmatizer' ]) kw_model = KeyBERT ( model = nlp ) Using spacy-transformer models: import spacy spacy . prefer_gpu () nlp = spacy . load ( \"en_core_web_trf\" , exclude = [ 'tagger' , 'parser' , 'ner' , 'attribute_ruler' , 'lemmatizer' ]) kw_model = KeyBERT ( model = nlp ) If you run into memory issues with spacy-transformer models, try: import spacy from thinc.api import set_gpu_allocator , require_gpu nlp = spacy . load ( \"en_core_web_trf\" , exclude = [ 'tagger' , 'parser' , 'ner' , 'attribute_ruler' , 'lemmatizer' ]) set_gpu_allocator ( \"pytorch\" ) require_gpu ( 0 ) kw_model = KeyBERT ( model = nlp )","title":"Spacy"},{"location":"guides/embeddings.html#universal-sentence-encoder-use","text":"The Universal Sentence Encoder encodes text into high dimensional vectors that are used here for embedding the documents. The model is trained and optimized for greater-than-word length text, such as sentences, phrases or short paragraphs. Using USE in KeyBERT is rather straightforward: import tensorflow_hub embedding_model = tensorflow_hub . load ( \"https://tfhub.dev/google/universal-sentence-encoder/4\" ) kw_model = KeyBERT ( model = embedding_model )","title":"Universal Sentence Encoder (USE)"},{"location":"guides/embeddings.html#gensim","text":"For Gensim, KeyBERT supports its gensim.downloader module. Here, we can download any model word embedding model to be used in KeyBERT. Note that Gensim is primarily used for Word Embedding models. This works typically best for short documents since the word embeddings are pooled. import gensim.downloader as api ft = api . load ( 'fasttext-wiki-news-subwords-300' ) kw_model = KeyBERT ( model = ft )","title":"Gensim"},{"location":"guides/embeddings.html#custom-backend","text":"If your backend or model cannot be found in the ones currently available, you can use the keybert.backend.BaseEmbedder class to create your own backend. Below, you will find an example of creating a SentenceTransformer backend for KeyBERT: from keybert.backend import BaseEmbedder from sentence_transformers import SentenceTransformer class CustomEmbedder ( BaseEmbedder ): def __init__ ( self , embedding_model ): super () . __init__ () self . embedding_model = embedding_model def embed ( self , documents , verbose = False ): embeddings = self . embedding_model . encode ( documents , show_progress_bar = verbose ) return embeddings # Create custom backend distilbert = SentenceTransformer ( \"paraphrase-MiniLM-L6-v2\" ) custom_embedder = CustomEmbedder ( embedding_model = distilbert ) # Pass custom backend to keybert kw_model = KeyBERT ( model = custom_embedder )","title":"Custom Backend"},{"location":"guides/quickstart.html","text":"Installation \u00b6 Installation can be done using pypi : pip install keybert You may want to install more depending on the transformers and language backends that you will be using. The possible installations are: pip install keybert[flair] pip install keybert[gensim] pip install keybert[spacy] pip install keybert[use] @font-face { font-family: \"Virgil\"; src: url(\"https://unpkg.com/@excalidraw/excalidraw@0.12.0/dist/excalidraw-assets/Virgil.woff2\"); } @font-face { font-family: \"Cascadia\"; src: url(\"https://unpkg.com/@excalidraw/excalidraw@0.12.0/dist/excalidraw-assets/Cascadia.woff2\"); } Input Document Tokenize Words Embed Tokens Extract Embeddings Embed Document Calculate Cosine Similarity Most microbats use echolocationto navigate and find food. Most microbats use echolocationto navigate and find food. most microbats use echolocation to navigate and find food 0.11 0.55 0.28 .... .... .... 0.72 0.96 0.34 most food Most microbats... most food ... ... .08 .73 We use the CountVectorizer from Scikit-Learn to tokenize our document into candidate kewords/keyphrases. We can use any language model that can embed both documents and keywords, like sentence-transformers. We calculate the cosine similarity between all candidate keywords and the input document. The keywords that have the largest similarity to the document are extracted. Basic usage \u00b6 The most minimal example can be seen below for the extraction of keywords: from keybert import KeyBERT doc = \"\"\" Supervised learning is the machine learning task of learning a function that maps an input to an output based on example input-output pairs.[1] It infers a function from labeled training data consisting of a set of training examples.[2] In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a 'reasonable' way (see inductive bias). \"\"\" kw_model = KeyBERT () keywords = kw_model . extract_keywords ( doc ) You can set keyphrase_ngram_range to set the length of the resulting keywords/keyphrases: >>> kw_model . extract_keywords ( doc , keyphrase_ngram_range = ( 1 , 1 ), stop_words = None ) [( 'learning' , 0.4604 ), ( 'algorithm' , 0.4556 ), ( 'training' , 0.4487 ), ( 'class' , 0.4086 ), ( 'mapping' , 0.3700 )] To extract keyphrases, simply set keyphrase_ngram_range to (1, 2) or higher depending on the number of words you would like in the resulting keyphrases: >>> kw_model . extract_keywords ( doc , keyphrase_ngram_range = ( 1 , 2 ), stop_words = None ) [( 'learning algorithm' , 0.6978 ), ( 'machine learning' , 0.6305 ), ( 'supervised learning' , 0.5985 ), ( 'algorithm analyzes' , 0.5860 ), ( 'learning function' , 0.5850 )] We can highlight the keywords in the document by simply setting highlight : keywords = kw_model . extract_keywords ( doc , highlight = True ) NOTE For a full overview of all possible transformer models see sentence-transformer . I would advise either \"all-MiniLM-L6-v2\" for English documents or \"paraphrase-multilingual-MiniLM-L12-v2\" for multi-lingual documents or any other language. Fine-tuning \u00b6 As a default, KeyBERT simply compares the documents and candidate keywords/keyphrases based on their cosine similarity. However, this might lead to very similar words ending up in the list of most accurate keywords/keyphrases. To make sure they are a bit more diversified, there are two approaches that we can take in order to fine-tune our output, Max Sum Distance and Maximal Marginal Relevance . Max Sum Distance \u00b6 To diversify the results, we take the 2 x top_n most similar words/phrases to the document. Then, we take all top_n combinations from the 2 x top_n words and extract the combination that are the least similar to each other by cosine similarity. >>> kw_model . extract_keywords ( doc , keyphrase_ngram_range = ( 3 , 3 ), stop_words = 'english' , use_maxsum = True , nr_candidates = 20 , top_n = 5 ) [( 'set training examples' , 0.7504 ), ( 'generalize training data' , 0.7727 ), ( 'requires learning algorithm' , 0.5050 ), ( 'supervised learning algorithm' , 0.3779 ), ( 'learning machine learning' , 0.2891 )] Maximal Marginal Relevance \u00b6 To diversify the results, we can use Maximal Margin Relevance (MMR) to create keywords / keyphrases which is also based on cosine similarity. The results with high diversity : kw_model . extract_keywords ( doc , keyphrase_ngram_range = ( 3 , 3 ), stop_words = 'english' , use_mmr = True , diversity = 0.7 ) [( 'algorithm generalize training' , 0.7727 ), ( 'labels unseen instances' , 0.1649 ), ( 'new examples optimal' , 0.4185 ), ( 'determine class labels' , 0.4774 ), ( 'supervised learning algorithm' , 0.7502 )] The results with low diversity : >>> kw_model . extract_keywords ( doc , keyphrase_ngram_range = ( 3 , 3 ), stop_words = 'english' , use_mmr = True , diversity = 0.2 ) [( 'algorithm generalize training' , 0.7727 ), ( 'supervised learning algorithm' , 0.7502 ), ( 'learning machine learning' , 0.7577 ), ( 'learning algorithm analyzes' , 0.7587 ), ( 'learning algorithm generalize' , 0.7514 )] Candidate Keywords/Keyphrases \u00b6 In some cases, one might want to be using candidate keywords generated by other keyword algorithms or retrieved from a select list of possible keywords/keyphrases. In KeyBERT, you can easily use those candidate keywords to perform keyword extraction: import yake from keybert import KeyBERT # Create candidates kw_extractor = yake . KeywordExtractor ( top = 50 ) candidates = kw_extractor . extract_keywords ( doc ) candidates = [ candidate [ 0 ] for candidate in candidates ] # Pass candidates to KeyBERT kw_model = KeyBERT () keywords = kw_model . extract_keywords ( doc , candidates = candidates ) Guided KeyBERT \u00b6 Guided KeyBERT is similar to Guided Topic Modeling in that it tries to steer the training towards a set of seeded terms. When applying KeyBERT it automatically extracts the most related keywords to a specific document. However, there are times when stakeholders and users are looking for specific types of keywords. For example, when publishing an article on your website through contentful, you typically already know the global keywords related to the article. However, there might be a specific topic in the article that you would like to be extracted through the keywords. To achieve this, we simply give KeyBERT a set of related seeded keywords (it can also be a single one!) and search for keywords that are similar to both the document and the seeded keywords. @font-face { font-family: \"Virgil\"; src: url(\"https://unpkg.com/@excalidraw/excalidraw@0.12.0/dist/excalidraw-assets/Virgil.woff2\"); } @font-face { font-family: \"Cascadia\"; src: url(\"https://unpkg.com/@excalidraw/excalidraw@0.12.0/dist/excalidraw-assets/Cascadia.woff2\"); } Input Document Tokenize Words Embed Tokens Extract Embeddings Average seed keyword and document embeddings Calculate Cosine Similarity Most microbats use echolocationto navigate and find food. Most microbats... sonar most microbats use echolocation to navigate and find food 0.11 0.55 0.32 0.28 .... .... .... .... 0.72 0.96 0.49 0.34 most food Most microbats... most food ... ... .08 .73 We use the CountVectorizer from Scikit-Learn to tokenize our document into candidate kewords/keyphrases. We embed the seeded keywords (e.g., the word \u201csonar\u201d) and calculate a weighted average with the document embedding (1:3). We calculate the cosine similarity between all candidate keywords and the input document. The keywords that have the largest similarity to the document are extracted. Using this feature is as simple as defining a list of seeded keywords and passing them to KeyBERT: from keybert import KeyBERT kw_model = KeyBERT () # Define our seeded term seed_keywords = [ \"information\" ] keywords = kw_model . extract_keywords ( doc , seed_keywords = seed_keywords ) Prepare embeddings \u00b6 When you have a large dataset and you want to fine-tune parameters such as diversity it can take quite a while to re-calculate the document and word embeddings each time you change a parameter. Instead, we can pre-calculate these embeddings and pass them to .extract_keywords such that we only have to calculate it once: from keybert import KeyBERT kw_model = KeyBERT () doc_embeddings , word_embeddings = kw_model . extract_embeddings ( docs ) You can then use these embeddings and pass them to .extract_keywords to speed up the tuning the model: keywords = kw_model . extract_keywords ( docs , doc_embeddings = doc_embeddings , word_embeddings = word_embeddings ) There are several parameters in .extract_embeddings that define how the list of candidate keywords/keyphrases is generated: candidates keyphrase_ngram_range stop_words min_df vectorizer The values of these parameters need to be exactly the same in .extract_embeddings as they are in . extract_keywords . In other words, the following will work as they use the same parameter subset: from keybert import KeyBERT kw_model = KeyBERT () doc_embeddings , word_embeddings = kw_model . extract_embeddings ( docs , min_df = 1 , stop_words = \"english\" ) keywords = kw_model . extract_keywords ( docs , min_df = 1 , stop_words = \"english\" , doc_embeddings = doc_embeddings , word_embeddings = word_embeddings ) The following, however, will throw an error since we did not use the same values for min_df and stop_words : from keybert import KeyBERT kw_model = KeyBERT () doc_embeddings , word_embeddings = kw_model . extract_embeddings ( docs , min_df = 3 , stop_words = \"dutch\" ) keywords = kw_model . extract_keywords ( docs , min_df = 1 , stop_words = \"english\" , doc_embeddings = doc_embeddings , word_embeddings = word_embeddings )","title":"Quickstart"},{"location":"guides/quickstart.html#installation","text":"Installation can be done using pypi : pip install keybert You may want to install more depending on the transformers and language backends that you will be using. The possible installations are: pip install keybert[flair] pip install keybert[gensim] pip install keybert[spacy] pip install keybert[use] @font-face { font-family: \"Virgil\"; src: url(\"https://unpkg.com/@excalidraw/excalidraw@0.12.0/dist/excalidraw-assets/Virgil.woff2\"); } @font-face { font-family: \"Cascadia\"; src: url(\"https://unpkg.com/@excalidraw/excalidraw@0.12.0/dist/excalidraw-assets/Cascadia.woff2\"); } Input Document Tokenize Words Embed Tokens Extract Embeddings Embed Document Calculate Cosine Similarity Most microbats use echolocationto navigate and find food. Most microbats use echolocationto navigate and find food. most microbats use echolocation to navigate and find food 0.11 0.55 0.28 .... .... .... 0.72 0.96 0.34 most food Most microbats... most food ... ... .08 .73 We use the CountVectorizer from Scikit-Learn to tokenize our document into candidate kewords/keyphrases. We can use any language model that can embed both documents and keywords, like sentence-transformers. We calculate the cosine similarity between all candidate keywords and the input document. The keywords that have the largest similarity to the document are extracted.","title":"Installation"},{"location":"guides/quickstart.html#basic-usage","text":"The most minimal example can be seen below for the extraction of keywords: from keybert import KeyBERT doc = \"\"\" Supervised learning is the machine learning task of learning a function that maps an input to an output based on example input-output pairs.[1] It infers a function from labeled training data consisting of a set of training examples.[2] In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a 'reasonable' way (see inductive bias). \"\"\" kw_model = KeyBERT () keywords = kw_model . extract_keywords ( doc ) You can set keyphrase_ngram_range to set the length of the resulting keywords/keyphrases: >>> kw_model . extract_keywords ( doc , keyphrase_ngram_range = ( 1 , 1 ), stop_words = None ) [( 'learning' , 0.4604 ), ( 'algorithm' , 0.4556 ), ( 'training' , 0.4487 ), ( 'class' , 0.4086 ), ( 'mapping' , 0.3700 )] To extract keyphrases, simply set keyphrase_ngram_range to (1, 2) or higher depending on the number of words you would like in the resulting keyphrases: >>> kw_model . extract_keywords ( doc , keyphrase_ngram_range = ( 1 , 2 ), stop_words = None ) [( 'learning algorithm' , 0.6978 ), ( 'machine learning' , 0.6305 ), ( 'supervised learning' , 0.5985 ), ( 'algorithm analyzes' , 0.5860 ), ( 'learning function' , 0.5850 )] We can highlight the keywords in the document by simply setting highlight : keywords = kw_model . extract_keywords ( doc , highlight = True ) NOTE For a full overview of all possible transformer models see sentence-transformer . I would advise either \"all-MiniLM-L6-v2\" for English documents or \"paraphrase-multilingual-MiniLM-L12-v2\" for multi-lingual documents or any other language.","title":"Basic usage"},{"location":"guides/quickstart.html#fine-tuning","text":"As a default, KeyBERT simply compares the documents and candidate keywords/keyphrases based on their cosine similarity. However, this might lead to very similar words ending up in the list of most accurate keywords/keyphrases. To make sure they are a bit more diversified, there are two approaches that we can take in order to fine-tune our output, Max Sum Distance and Maximal Marginal Relevance .","title":"Fine-tuning"},{"location":"guides/quickstart.html#max-sum-distance","text":"To diversify the results, we take the 2 x top_n most similar words/phrases to the document. Then, we take all top_n combinations from the 2 x top_n words and extract the combination that are the least similar to each other by cosine similarity. >>> kw_model . extract_keywords ( doc , keyphrase_ngram_range = ( 3 , 3 ), stop_words = 'english' , use_maxsum = True , nr_candidates = 20 , top_n = 5 ) [( 'set training examples' , 0.7504 ), ( 'generalize training data' , 0.7727 ), ( 'requires learning algorithm' , 0.5050 ), ( 'supervised learning algorithm' , 0.3779 ), ( 'learning machine learning' , 0.2891 )]","title":"Max Sum Distance"},{"location":"guides/quickstart.html#maximal-marginal-relevance","text":"To diversify the results, we can use Maximal Margin Relevance (MMR) to create keywords / keyphrases which is also based on cosine similarity. The results with high diversity : kw_model . extract_keywords ( doc , keyphrase_ngram_range = ( 3 , 3 ), stop_words = 'english' , use_mmr = True , diversity = 0.7 ) [( 'algorithm generalize training' , 0.7727 ), ( 'labels unseen instances' , 0.1649 ), ( 'new examples optimal' , 0.4185 ), ( 'determine class labels' , 0.4774 ), ( 'supervised learning algorithm' , 0.7502 )] The results with low diversity : >>> kw_model . extract_keywords ( doc , keyphrase_ngram_range = ( 3 , 3 ), stop_words = 'english' , use_mmr = True , diversity = 0.2 ) [( 'algorithm generalize training' , 0.7727 ), ( 'supervised learning algorithm' , 0.7502 ), ( 'learning machine learning' , 0.7577 ), ( 'learning algorithm analyzes' , 0.7587 ), ( 'learning algorithm generalize' , 0.7514 )]","title":"Maximal Marginal Relevance"},{"location":"guides/quickstart.html#candidate-keywordskeyphrases","text":"In some cases, one might want to be using candidate keywords generated by other keyword algorithms or retrieved from a select list of possible keywords/keyphrases. In KeyBERT, you can easily use those candidate keywords to perform keyword extraction: import yake from keybert import KeyBERT # Create candidates kw_extractor = yake . KeywordExtractor ( top = 50 ) candidates = kw_extractor . extract_keywords ( doc ) candidates = [ candidate [ 0 ] for candidate in candidates ] # Pass candidates to KeyBERT kw_model = KeyBERT () keywords = kw_model . extract_keywords ( doc , candidates = candidates )","title":"Candidate Keywords/Keyphrases"},{"location":"guides/quickstart.html#guided-keybert","text":"Guided KeyBERT is similar to Guided Topic Modeling in that it tries to steer the training towards a set of seeded terms. When applying KeyBERT it automatically extracts the most related keywords to a specific document. However, there are times when stakeholders and users are looking for specific types of keywords. For example, when publishing an article on your website through contentful, you typically already know the global keywords related to the article. However, there might be a specific topic in the article that you would like to be extracted through the keywords. To achieve this, we simply give KeyBERT a set of related seeded keywords (it can also be a single one!) and search for keywords that are similar to both the document and the seeded keywords. @font-face { font-family: \"Virgil\"; src: url(\"https://unpkg.com/@excalidraw/excalidraw@0.12.0/dist/excalidraw-assets/Virgil.woff2\"); } @font-face { font-family: \"Cascadia\"; src: url(\"https://unpkg.com/@excalidraw/excalidraw@0.12.0/dist/excalidraw-assets/Cascadia.woff2\"); } Input Document Tokenize Words Embed Tokens Extract Embeddings Average seed keyword and document embeddings Calculate Cosine Similarity Most microbats use echolocationto navigate and find food. Most microbats... sonar most microbats use echolocation to navigate and find food 0.11 0.55 0.32 0.28 .... .... .... .... 0.72 0.96 0.49 0.34 most food Most microbats... most food ... ... .08 .73 We use the CountVectorizer from Scikit-Learn to tokenize our document into candidate kewords/keyphrases. We embed the seeded keywords (e.g., the word \u201csonar\u201d) and calculate a weighted average with the document embedding (1:3). We calculate the cosine similarity between all candidate keywords and the input document. The keywords that have the largest similarity to the document are extracted. Using this feature is as simple as defining a list of seeded keywords and passing them to KeyBERT: from keybert import KeyBERT kw_model = KeyBERT () # Define our seeded term seed_keywords = [ \"information\" ] keywords = kw_model . extract_keywords ( doc , seed_keywords = seed_keywords )","title":"Guided KeyBERT"},{"location":"guides/quickstart.html#prepare-embeddings","text":"When you have a large dataset and you want to fine-tune parameters such as diversity it can take quite a while to re-calculate the document and word embeddings each time you change a parameter. Instead, we can pre-calculate these embeddings and pass them to .extract_keywords such that we only have to calculate it once: from keybert import KeyBERT kw_model = KeyBERT () doc_embeddings , word_embeddings = kw_model . extract_embeddings ( docs ) You can then use these embeddings and pass them to .extract_keywords to speed up the tuning the model: keywords = kw_model . extract_keywords ( docs , doc_embeddings = doc_embeddings , word_embeddings = word_embeddings ) There are several parameters in .extract_embeddings that define how the list of candidate keywords/keyphrases is generated: candidates keyphrase_ngram_range stop_words min_df vectorizer The values of these parameters need to be exactly the same in .extract_embeddings as they are in . extract_keywords . In other words, the following will work as they use the same parameter subset: from keybert import KeyBERT kw_model = KeyBERT () doc_embeddings , word_embeddings = kw_model . extract_embeddings ( docs , min_df = 1 , stop_words = \"english\" ) keywords = kw_model . extract_keywords ( docs , min_df = 1 , stop_words = \"english\" , doc_embeddings = doc_embeddings , word_embeddings = word_embeddings ) The following, however, will throw an error since we did not use the same values for min_df and stop_words : from keybert import KeyBERT kw_model = KeyBERT () doc_embeddings , word_embeddings = kw_model . extract_embeddings ( docs , min_df = 3 , stop_words = \"dutch\" ) keywords = kw_model . extract_keywords ( docs , min_df = 1 , stop_words = \"english\" , doc_embeddings = doc_embeddings , word_embeddings = word_embeddings )","title":"Prepare embeddings"}]}