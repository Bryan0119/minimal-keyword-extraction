
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Leveraging BERT to extract important keywords">
      
      
        <meta name="author" content="Maarten P. Grootendorst">
      
      
        <link rel="canonical" href="https://maartengr.github.io/keyBERT/api/keybert.html">
      
      <link rel="icon" href="../icon.png">
      <meta name="generator" content="mkdocs-1.3.0, mkdocs-material-8.2.8">
    
    
      
        <title>KeyBERT - KeyBERT</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.644de097.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.e6a45f82.min.css">
        
          
          
          <meta name="theme-color" content="#000000">
        
      
    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:300,300i,400,400i,700,700i%7CUbuntu+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Ubuntu";--md-code-font:"Ubuntu Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../style.css">
    
    <script>__md_scope=new URL("..",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="black" data-md-color-accent="blue">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#keybert" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../index.html" title="KeyBERT" class="md-header__button md-logo" aria-label="KeyBERT" data-md-component="logo">
      
  <img src="../icon.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            KeyBERT
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              KeyBERT
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/MaartenGr/keyBERT" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../index.html" title="KeyBERT" class="md-nav__button md-logo" aria-label="KeyBERT" data-md-component="logo">
      
  <img src="../icon.png" alt="logo">

    </a>
    KeyBERT
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/MaartenGr/keyBERT" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../index.html" class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2">
          Guides
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Guides" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          Guides
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../guides/quickstart.html" class="md-nav__link">
        Quickstart
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../guides/embeddings.html" class="md-nav__link">
        Embedding Models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../guides/countvectorizer.html" class="md-nav__link">
        CountVectorizer
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_3">
          API
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="API" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          API
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          KeyBERT
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="keybert.html" class="md-nav__link md-nav__link--active">
        KeyBERT
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#keybert._model.KeyBERT" class="md-nav__link">
    keybert._model.KeyBERT
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#keybert._model.KeyBERT.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#keybert._model.KeyBERT.extract_keywords" class="md-nav__link">
    extract_keywords()
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="mmr.html" class="md-nav__link">
        MMR
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="maxsum.html" class="md-nav__link">
        MaxSum
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../faq.html" class="md-nav__link">
        FAQ
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../changelog.html" class="md-nav__link">
        Changelog
      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#keybert._model.KeyBERT" class="md-nav__link">
    keybert._model.KeyBERT
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#keybert._model.KeyBERT.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#keybert._model.KeyBERT.extract_keywords" class="md-nav__link">
    extract_keywords()
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
<a href="https://github.com/MaartenGr/keyBERT/edit/master/docs/api/keybert.md" title="Edit this page" class="md-content__button md-icon">
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg>
</a>



<h1 id="keybert"><code>KeyBERT</code><a class="headerlink" href="#keybert" title="Permanent link">&para;</a></h1>


  <div class="doc doc-object doc-class">

<a id="keybert._model.KeyBERT"></a>
    <div class="doc doc-contents first">

      <p>A minimal method for keyword extraction with BERT</p>
<p>The keyword extraction is done by finding the sub-phrases in
a document that are the most similar to the document itself.</p>
<p>First, document embeddings are extracted with BERT to get a
document-level representation. Then, word embeddings are extracted
for N-gram words/phrases. Finally, we use cosine similarity to find the
words/phrases that are the most similar to the document.</p>
<p>The most similar words could then be identified as the words that
best describe the entire document.</p>

        <details class="quote">
          <summary>Source code in <code>keybert\_model.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">KeyBERT</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A minimal method for keyword extraction with BERT</span>

<span class="sd">    The keyword extraction is done by finding the sub-phrases in</span>
<span class="sd">    a document that are the most similar to the document itself.</span>

<span class="sd">    First, document embeddings are extracted with BERT to get a</span>
<span class="sd">    document-level representation. Then, word embeddings are extracted</span>
<span class="sd">    for N-gram words/phrases. Finally, we use cosine similarity to find the</span>
<span class="sd">    words/phrases that are the most similar to the document.</span>

<span class="sd">    The most similar words could then be identified as the words that</span>
<span class="sd">    best describe the entire document.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">&quot;all-MiniLM-L6-v2&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;KeyBERT initialization</span>

<span class="sd">        Arguments:</span>
<span class="sd">            model: Use a custom embedding model.</span>
<span class="sd">                   The following backends are currently supported:</span>
<span class="sd">                      * SentenceTransformers</span>
<span class="sd">                      * Flair</span>
<span class="sd">                      * Spacy</span>
<span class="sd">                      * Gensim</span>
<span class="sd">                      * USE (TF-Hub)</span>
<span class="sd">                    You can also pass in a string that points to one of the following</span>
<span class="sd">                    sentence-transformers models:</span>
<span class="sd">                      * https://www.sbert.net/docs/pretrained_models.html</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">select_backend</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">extract_keywords</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">docs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span>
        <span class="n">candidates</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">keyphrase_ngram_range</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="n">stop_words</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="s2">&quot;english&quot;</span><span class="p">,</span>
        <span class="n">top_n</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
        <span class="n">min_df</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">use_maxsum</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">use_mmr</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">diversity</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span>
        <span class="n">nr_candidates</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
        <span class="n">vectorizer</span><span class="p">:</span> <span class="n">CountVectorizer</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">highlight</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">seed_keywords</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]],</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]]]:</span>
        <span class="sd">&quot;&quot;&quot;Extract keywords and/or keyphrases</span>

<span class="sd">        I would advise you to iterate over single documents as they</span>
<span class="sd">        will need the least amount of memory. Even though this is slower,</span>
<span class="sd">        you are not likely to run into memory errors.</span>

<span class="sd">        There is an option to extract keywords for multiple documents</span>
<span class="sd">        that is faster than extraction for multiple single documents.</span>
<span class="sd">        However, this method assumes that you can keep the word embeddings</span>
<span class="sd">        for all words in the vocabulary in memory which might be troublesome.</span>
<span class="sd">        I would advise against using this option and simply iterating</span>
<span class="sd">        over documents instead if you have limited hardware.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            docs: The document(s) for which to extract keywords/keyphrases</span>
<span class="sd">            candidates: Candidate keywords/keyphrases to use instead of extracting them from the document(s)</span>
<span class="sd">            keyphrase_ngram_range: Length, in words, of the extracted keywords/keyphrases</span>
<span class="sd">            stop_words: Stopwords to remove from the document</span>
<span class="sd">            top_n: Return the top n keywords/keyphrases</span>
<span class="sd">            min_df: Minimum document frequency of a word across all documents</span>
<span class="sd">                    if keywords for multiple documents need to be extracted</span>
<span class="sd">            use_maxsum: Whether to use Max Sum Similarity for the selection</span>
<span class="sd">                        of keywords/keyphrases</span>
<span class="sd">            use_mmr: Whether to use Maximal Marginal Relevance (MMR) for the</span>
<span class="sd">                     selection of keywords/keyphrases</span>
<span class="sd">            diversity: The diversity of the results between 0 and 1 if use_mmr</span>
<span class="sd">                       is set to True</span>
<span class="sd">            nr_candidates: The number of candidates to consider if use_maxsum is</span>
<span class="sd">                           set to True</span>
<span class="sd">            vectorizer: Pass in your own CountVectorizer from scikit-learn</span>
<span class="sd">            highlight: Whether to print the document and highlight</span>
<span class="sd">                       its keywords/keyphrases. NOTE: This does not work if</span>
<span class="sd">                       multiple documents are passed.</span>
<span class="sd">            seed_keywords: Seed keywords that may guide the extraction of keywords by</span>
<span class="sd">                           steering the similarities towards the seeded keywords</span>

<span class="sd">        Returns:</span>
<span class="sd">            keywords: the top n keywords for a document with their respective distances</span>
<span class="sd">                      to the input document</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">keywords</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_extract_keywords_single_doc</span><span class="p">(</span>
                <span class="n">doc</span><span class="o">=</span><span class="n">docs</span><span class="p">,</span>
                <span class="n">candidates</span><span class="o">=</span><span class="n">candidates</span><span class="p">,</span>
                <span class="n">keyphrase_ngram_range</span><span class="o">=</span><span class="n">keyphrase_ngram_range</span><span class="p">,</span>
                <span class="n">stop_words</span><span class="o">=</span><span class="n">stop_words</span><span class="p">,</span>
                <span class="n">top_n</span><span class="o">=</span><span class="n">top_n</span><span class="p">,</span>
                <span class="n">use_maxsum</span><span class="o">=</span><span class="n">use_maxsum</span><span class="p">,</span>
                <span class="n">use_mmr</span><span class="o">=</span><span class="n">use_mmr</span><span class="p">,</span>
                <span class="n">diversity</span><span class="o">=</span><span class="n">diversity</span><span class="p">,</span>
                <span class="n">nr_candidates</span><span class="o">=</span><span class="n">nr_candidates</span><span class="p">,</span>
                <span class="n">vectorizer</span><span class="o">=</span><span class="n">vectorizer</span><span class="p">,</span>
                <span class="n">seed_keywords</span><span class="o">=</span><span class="n">seed_keywords</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">highlight</span><span class="p">:</span>
                <span class="n">highlight_document</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">keywords</span><span class="p">)</span>

            <span class="k">return</span> <span class="n">keywords</span>

        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;Although extracting keywords for multiple documents is faster &quot;</span>
                <span class="s2">&quot;than iterating over single documents, it requires significantly more memory &quot;</span>
                <span class="s2">&quot;to hold all word embeddings. Use this at your own discretion!&quot;</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_extract_keywords_multiple_docs</span><span class="p">(</span>
                <span class="n">docs</span><span class="p">,</span> <span class="n">keyphrase_ngram_range</span><span class="p">,</span> <span class="n">stop_words</span><span class="p">,</span> <span class="n">top_n</span><span class="p">,</span> <span class="n">min_df</span><span class="p">,</span> <span class="n">vectorizer</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_extract_keywords_single_doc</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">doc</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">candidates</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">keyphrase_ngram_range</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="n">stop_words</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="s2">&quot;english&quot;</span><span class="p">,</span>
        <span class="n">top_n</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
        <span class="n">use_maxsum</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">use_mmr</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">diversity</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span>
        <span class="n">nr_candidates</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
        <span class="n">vectorizer</span><span class="p">:</span> <span class="n">CountVectorizer</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">seed_keywords</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]:</span>
        <span class="sd">&quot;&quot;&quot;Extract keywords/keyphrases for a single document</span>

<span class="sd">        Arguments:</span>
<span class="sd">            doc: The document for which to extract keywords/keyphrases</span>
<span class="sd">            candidates: Candidate keywords/keyphrases to use instead of extracting them from the document(s)</span>
<span class="sd">            keyphrase_ngram_range: Length, in words, of the extracted keywords/keyphrases</span>
<span class="sd">            stop_words: Stopwords to remove from the document</span>
<span class="sd">            top_n: Return the top n keywords/keyphrases</span>
<span class="sd">            use_mmr: Whether to use Max Sum Similarity</span>
<span class="sd">            use_mmr: Whether to use MMR</span>
<span class="sd">            diversity: The diversity of results between 0 and 1 if use_mmr is True</span>
<span class="sd">            nr_candidates: The number of candidates to consider if use_maxsum is set to True</span>
<span class="sd">            vectorizer: Pass in your own CountVectorizer from scikit-learn</span>
<span class="sd">            seed_keywords: Seed keywords that may guide the extraction of keywords by</span>
<span class="sd">                           steering the similarities towards the seeded keywords</span>

<span class="sd">        Returns:</span>
<span class="sd">            keywords: the top n keywords for a document with their respective distances</span>
<span class="sd">                      to the input document</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Extract Words</span>
            <span class="k">if</span> <span class="n">candidates</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">vectorizer</span><span class="p">:</span>
                    <span class="n">count</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="n">doc</span><span class="p">])</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">count</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span>
                        <span class="n">ngram_range</span><span class="o">=</span><span class="n">keyphrase_ngram_range</span><span class="p">,</span> <span class="n">stop_words</span><span class="o">=</span><span class="n">stop_words</span>
                    <span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="n">doc</span><span class="p">])</span>
                <span class="n">candidates</span> <span class="o">=</span> <span class="n">count</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">()</span>

            <span class="c1"># Extract Embeddings</span>
            <span class="n">doc_embedding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">embed</span><span class="p">([</span><span class="n">doc</span><span class="p">])</span>
            <span class="n">candidate_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="n">candidates</span><span class="p">)</span>

            <span class="c1"># Guided KeyBERT with seed keywords</span>
            <span class="k">if</span> <span class="n">seed_keywords</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">seed_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">embed</span><span class="p">([</span><span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">seed_keywords</span><span class="p">)])</span>
                <span class="n">doc_embedding</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span>
                    <span class="p">[</span><span class="n">doc_embedding</span><span class="p">,</span> <span class="n">seed_embeddings</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
                <span class="p">)</span>

            <span class="c1"># Calculate distances and extract keywords</span>
            <span class="k">if</span> <span class="n">use_mmr</span><span class="p">:</span>
                <span class="n">keywords</span> <span class="o">=</span> <span class="n">mmr</span><span class="p">(</span>
                    <span class="n">doc_embedding</span><span class="p">,</span> <span class="n">candidate_embeddings</span><span class="p">,</span> <span class="n">candidates</span><span class="p">,</span> <span class="n">top_n</span><span class="p">,</span> <span class="n">diversity</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="n">use_maxsum</span><span class="p">:</span>
                <span class="n">keywords</span> <span class="o">=</span> <span class="n">max_sum_similarity</span><span class="p">(</span>
                    <span class="n">doc_embedding</span><span class="p">,</span>
                    <span class="n">candidate_embeddings</span><span class="p">,</span>
                    <span class="n">candidates</span><span class="p">,</span>
                    <span class="n">top_n</span><span class="p">,</span>
                    <span class="n">nr_candidates</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">distances</span> <span class="o">=</span> <span class="n">cosine_similarity</span><span class="p">(</span><span class="n">doc_embedding</span><span class="p">,</span> <span class="n">candidate_embeddings</span><span class="p">)</span>
                <span class="n">keywords</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="p">(</span><span class="n">candidates</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="nb">round</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">distances</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">index</span><span class="p">]),</span> <span class="mi">4</span><span class="p">))</span>
                    <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">distances</span><span class="o">.</span><span class="n">argsort</span><span class="p">()[</span><span class="mi">0</span><span class="p">][</span><span class="o">-</span><span class="n">top_n</span><span class="p">:]</span>
                <span class="p">][::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

            <span class="k">return</span> <span class="n">keywords</span>
        <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">_extract_keywords_multiple_docs</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">docs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">keyphrase_ngram_range</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="n">stop_words</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;english&quot;</span><span class="p">,</span>
        <span class="n">top_n</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
        <span class="n">min_df</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">vectorizer</span><span class="p">:</span> <span class="n">CountVectorizer</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]]:</span>
        <span class="sd">&quot;&quot;&quot;Extract keywords/keyphrases for a multiple documents</span>

<span class="sd">        This currently does not use MMR and Max Sum Similarity as it cannot</span>
<span class="sd">        process these methods in bulk.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            docs: The document for which to extract keywords/keyphrases</span>
<span class="sd">            keyphrase_ngram_range: Length, in words, of the extracted keywords/keyphrases</span>
<span class="sd">            stop_words: Stopwords to remove from the document</span>
<span class="sd">            top_n: Return the top n keywords/keyphrases</span>
<span class="sd">            min_df: The minimum frequency of words</span>
<span class="sd">            vectorizer: Pass in your own CountVectorizer from scikit-learn</span>

<span class="sd">        Returns:</span>
<span class="sd">            keywords: the top n keywords for a document with their respective distances</span>
<span class="sd">                      to the input document</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Extract words</span>
        <span class="k">if</span> <span class="n">vectorizer</span><span class="p">:</span>
            <span class="n">count</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">count</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span>
                <span class="n">ngram_range</span><span class="o">=</span><span class="n">keyphrase_ngram_range</span><span class="p">,</span> <span class="n">stop_words</span><span class="o">=</span><span class="n">stop_words</span><span class="p">,</span> <span class="n">min_df</span><span class="o">=</span><span class="n">min_df</span>
            <span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
        <span class="n">words</span> <span class="o">=</span> <span class="n">count</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">()</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">count</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>

        <span class="c1"># Extract embeddings</span>
        <span class="n">doc_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
        <span class="n">word_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>

        <span class="c1"># Extract keywords</span>
        <span class="n">keywords</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="n">docs</span><span class="p">)):</span>
            <span class="n">doc_words</span> <span class="o">=</span> <span class="p">[</span><span class="n">words</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">df</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()[</span><span class="mi">1</span><span class="p">]]</span>

            <span class="k">if</span> <span class="n">doc_words</span><span class="p">:</span>
                <span class="n">doc_word_embeddings</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
                    <span class="p">[</span><span class="n">word_embeddings</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">df</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()[</span><span class="mi">1</span><span class="p">]]</span>
                <span class="p">)</span>
                <span class="n">distances</span> <span class="o">=</span> <span class="n">cosine_similarity</span><span class="p">(</span>
                    <span class="p">[</span><span class="n">doc_embeddings</span><span class="p">[</span><span class="n">index</span><span class="p">]],</span> <span class="n">doc_word_embeddings</span>
                <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">doc_keywords</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="p">(</span><span class="n">doc_words</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="nb">round</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">distances</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="mi">4</span><span class="p">))</span>
                    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">distances</span><span class="o">.</span><span class="n">argsort</span><span class="p">()[</span><span class="o">-</span><span class="n">top_n</span><span class="p">:]</span>
                <span class="p">]</span>
                <span class="n">keywords</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">doc_keywords</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">keywords</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="s2">&quot;None Found&quot;</span><span class="p">])</span>

        <span class="k">return</span> <span class="n">keywords</span>
</code></pre></div>
        </details>



  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h2 id="keybert._model.KeyBERT.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s1">&#39;all-MiniLM-L6-v2&#39;</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

<a href="#keybert._model.KeyBERT.__init__" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents ">

      <p>KeyBERT initialization</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>model</code></td>
        <td></td>
        <td><p>Use a custom embedding model.
   The following backends are currently supported:
      * SentenceTransformers
      * Flair
      * Spacy
      * Gensim
      * USE (TF-Hub)
    You can also pass in a string that points to one of the following
    sentence-transformers models:
      * https://www.sbert.net/docs/pretrained_models.html</p></td>
        <td><code>&#39;all-MiniLM-L6-v2&#39;</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>keybert\_model.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">&quot;all-MiniLM-L6-v2&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;KeyBERT initialization</span>

<span class="sd">    Arguments:</span>
<span class="sd">        model: Use a custom embedding model.</span>
<span class="sd">               The following backends are currently supported:</span>
<span class="sd">                  * SentenceTransformers</span>
<span class="sd">                  * Flair</span>
<span class="sd">                  * Spacy</span>
<span class="sd">                  * Gensim</span>
<span class="sd">                  * USE (TF-Hub)</span>
<span class="sd">                You can also pass in a string that points to one of the following</span>
<span class="sd">                sentence-transformers models:</span>
<span class="sd">                  * https://www.sbert.net/docs/pretrained_models.html</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">select_backend</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h2 id="keybert._model.KeyBERT.extract_keywords" class="doc doc-heading">
<code class="highlight language-python"><span class="n">extract_keywords</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">docs</span><span class="p">,</span> <span class="n">candidates</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keyphrase_ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">stop_words</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">,</span> <span class="n">top_n</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">min_df</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">use_maxsum</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_mmr</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">diversity</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">nr_candidates</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">vectorizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">highlight</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">seed_keywords</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


<a href="#keybert._model.KeyBERT.extract_keywords" class="headerlink" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents ">

      <p>Extract keywords and/or keyphrases</p>
<p>I would advise you to iterate over single documents as they
will need the least amount of memory. Even though this is slower,
you are not likely to run into memory errors.</p>
<p>There is an option to extract keywords for multiple documents
that is faster than extraction for multiple single documents.
However, this method assumes that you can keep the word embeddings
for all words in the vocabulary in memory which might be troublesome.
I would advise against using this option and simply iterating
over documents instead if you have limited hardware.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>docs</code></td>
        <td><code>Union[str, List[str]]</code></td>
        <td><p>The document(s) for which to extract keywords/keyphrases</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>candidates</code></td>
        <td><code>List[str]</code></td>
        <td><p>Candidate keywords/keyphrases to use instead of extracting them from the document(s)</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>keyphrase_ngram_range</code></td>
        <td><code>Tuple[int, int]</code></td>
        <td><p>Length, in words, of the extracted keywords/keyphrases</p></td>
        <td><code>(1, 1)</code></td>
      </tr>
      <tr>
        <td><code>stop_words</code></td>
        <td><code>Union[str, List[str]]</code></td>
        <td><p>Stopwords to remove from the document</p></td>
        <td><code>&#39;english&#39;</code></td>
      </tr>
      <tr>
        <td><code>top_n</code></td>
        <td><code>int</code></td>
        <td><p>Return the top n keywords/keyphrases</p></td>
        <td><code>5</code></td>
      </tr>
      <tr>
        <td><code>min_df</code></td>
        <td><code>int</code></td>
        <td><p>Minimum document frequency of a word across all documents
    if keywords for multiple documents need to be extracted</p></td>
        <td><code>1</code></td>
      </tr>
      <tr>
        <td><code>use_maxsum</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to use Max Sum Similarity for the selection
        of keywords/keyphrases</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>use_mmr</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to use Maximal Marginal Relevance (MMR) for the
     selection of keywords/keyphrases</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>diversity</code></td>
        <td><code>float</code></td>
        <td><p>The diversity of the results between 0 and 1 if use_mmr
       is set to True</p></td>
        <td><code>0.5</code></td>
      </tr>
      <tr>
        <td><code>nr_candidates</code></td>
        <td><code>int</code></td>
        <td><p>The number of candidates to consider if use_maxsum is
           set to True</p></td>
        <td><code>20</code></td>
      </tr>
      <tr>
        <td><code>vectorizer</code></td>
        <td><code>CountVectorizer</code></td>
        <td><p>Pass in your own CountVectorizer from scikit-learn</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>highlight</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to print the document and highlight
       its keywords/keyphrases. NOTE: This does not work if
       multiple documents are passed.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>seed_keywords</code></td>
        <td><code>List[str]</code></td>
        <td><p>Seed keywords that may guide the extraction of keywords by
           steering the similarities towards the seeded keywords</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>keywords</code></td>
      <td><p>the top n keywords for a document with their respective distances
          to the input document</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>keybert\_model.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">extract_keywords</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">docs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span>
    <span class="n">candidates</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">keyphrase_ngram_range</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">stop_words</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="s2">&quot;english&quot;</span><span class="p">,</span>
    <span class="n">top_n</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
    <span class="n">min_df</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">use_maxsum</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">use_mmr</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">diversity</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span>
    <span class="n">nr_candidates</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
    <span class="n">vectorizer</span><span class="p">:</span> <span class="n">CountVectorizer</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">highlight</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">seed_keywords</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]],</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]]]:</span>
    <span class="sd">&quot;&quot;&quot;Extract keywords and/or keyphrases</span>

<span class="sd">    I would advise you to iterate over single documents as they</span>
<span class="sd">    will need the least amount of memory. Even though this is slower,</span>
<span class="sd">    you are not likely to run into memory errors.</span>

<span class="sd">    There is an option to extract keywords for multiple documents</span>
<span class="sd">    that is faster than extraction for multiple single documents.</span>
<span class="sd">    However, this method assumes that you can keep the word embeddings</span>
<span class="sd">    for all words in the vocabulary in memory which might be troublesome.</span>
<span class="sd">    I would advise against using this option and simply iterating</span>
<span class="sd">    over documents instead if you have limited hardware.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        docs: The document(s) for which to extract keywords/keyphrases</span>
<span class="sd">        candidates: Candidate keywords/keyphrases to use instead of extracting them from the document(s)</span>
<span class="sd">        keyphrase_ngram_range: Length, in words, of the extracted keywords/keyphrases</span>
<span class="sd">        stop_words: Stopwords to remove from the document</span>
<span class="sd">        top_n: Return the top n keywords/keyphrases</span>
<span class="sd">        min_df: Minimum document frequency of a word across all documents</span>
<span class="sd">                if keywords for multiple documents need to be extracted</span>
<span class="sd">        use_maxsum: Whether to use Max Sum Similarity for the selection</span>
<span class="sd">                    of keywords/keyphrases</span>
<span class="sd">        use_mmr: Whether to use Maximal Marginal Relevance (MMR) for the</span>
<span class="sd">                 selection of keywords/keyphrases</span>
<span class="sd">        diversity: The diversity of the results between 0 and 1 if use_mmr</span>
<span class="sd">                   is set to True</span>
<span class="sd">        nr_candidates: The number of candidates to consider if use_maxsum is</span>
<span class="sd">                       set to True</span>
<span class="sd">        vectorizer: Pass in your own CountVectorizer from scikit-learn</span>
<span class="sd">        highlight: Whether to print the document and highlight</span>
<span class="sd">                   its keywords/keyphrases. NOTE: This does not work if</span>
<span class="sd">                   multiple documents are passed.</span>
<span class="sd">        seed_keywords: Seed keywords that may guide the extraction of keywords by</span>
<span class="sd">                       steering the similarities towards the seeded keywords</span>

<span class="sd">    Returns:</span>
<span class="sd">        keywords: the top n keywords for a document with their respective distances</span>
<span class="sd">                  to the input document</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">keywords</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_extract_keywords_single_doc</span><span class="p">(</span>
            <span class="n">doc</span><span class="o">=</span><span class="n">docs</span><span class="p">,</span>
            <span class="n">candidates</span><span class="o">=</span><span class="n">candidates</span><span class="p">,</span>
            <span class="n">keyphrase_ngram_range</span><span class="o">=</span><span class="n">keyphrase_ngram_range</span><span class="p">,</span>
            <span class="n">stop_words</span><span class="o">=</span><span class="n">stop_words</span><span class="p">,</span>
            <span class="n">top_n</span><span class="o">=</span><span class="n">top_n</span><span class="p">,</span>
            <span class="n">use_maxsum</span><span class="o">=</span><span class="n">use_maxsum</span><span class="p">,</span>
            <span class="n">use_mmr</span><span class="o">=</span><span class="n">use_mmr</span><span class="p">,</span>
            <span class="n">diversity</span><span class="o">=</span><span class="n">diversity</span><span class="p">,</span>
            <span class="n">nr_candidates</span><span class="o">=</span><span class="n">nr_candidates</span><span class="p">,</span>
            <span class="n">vectorizer</span><span class="o">=</span><span class="n">vectorizer</span><span class="p">,</span>
            <span class="n">seed_keywords</span><span class="o">=</span><span class="n">seed_keywords</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">highlight</span><span class="p">:</span>
            <span class="n">highlight_document</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">keywords</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">keywords</span>

    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;Although extracting keywords for multiple documents is faster &quot;</span>
            <span class="s2">&quot;than iterating over single documents, it requires significantly more memory &quot;</span>
            <span class="s2">&quot;to hold all word embeddings. Use this at your own discretion!&quot;</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_extract_keywords_multiple_docs</span><span class="p">(</span>
            <span class="n">docs</span><span class="p">,</span> <span class="n">keyphrase_ngram_range</span><span class="p">,</span> <span class="n">stop_words</span><span class="p">,</span> <span class="n">top_n</span><span class="p">,</span> <span class="n">min_df</span><span class="p">,</span> <span class="n">vectorizer</span>
        <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>

              
            </article>
          </div>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="Footer">
      
        
        <a href="../guides/countvectorizer.html" class="md-footer__link md-footer__link--prev" aria-label="Previous: CountVectorizer" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              CountVectorizer
            </div>
          </div>
        </a>
      
      
        
        <a href="mmr.html" class="md-footer__link md-footer__link--next" aria-label="Next: MMR" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              MMR
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2021 Maintained by <a href="https://github.com/MaartenGr">Maarten</a>.
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.5e67fbfe.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.c44cc438.min.js"></script>
      
    
  </body>
</html>